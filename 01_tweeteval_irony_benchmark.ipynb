{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5c88635",
   "metadata": {},
   "source": [
    "# Irony Detection Benchmark on TweetEval\n",
    "\n",
    "## Objective\n",
    "\n",
    "This notebook benchmarks several approaches for irony detection on Twitter\n",
    "using the TweetEval Irony dataset.\n",
    "\n",
    "We evaluate models based on a trade-off between **performance** (F1-Score on the Irony class) and **inference efficiency** (VRAM usage, Latency).\n",
    "\n",
    "**Approaches Compared:**\n",
    "1.  **State-of-the-Art (Reference):** `cardiffnlp/twitter-roberta-base-2021-124m-irony` \n",
    "2.  **Efficient Fine-Tuning (Challenger):** `SetFit` with `sentence-transformers/paraphrase-mpnet-base-v2`\n",
    "3.  **Classic ML + Embeddings:** `XGBoost` on `Jina-Embeddings-v3` (dim reduced)\n",
    "4.  **LLM (Baseline):** `Qwen 2.5 7B` (Local inference) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7c94b5",
   "metadata": {},
   "source": [
    "## Dataset: TweetEval (Irony)\n",
    "\n",
    "We use the official **TweetEval** benchmark (Irony subset), derived from *SemEval-2018 Task 3*.  \n",
    "It consists of tweets labelled as **0 (Non-Irony)** or **1 (Irony)**.\n",
    "\n",
    "**Data Splits & Distribution:**\n",
    "\n",
    "| Split | Total Samples | Non-Irony (0) | Irony (1) | Balance (Irony %) |\n",
    "| :--- | :---: | :---: | :---: | :---: |\n",
    "| **Train** | 2,862 | ~1,430 | ~1,432 | ~50% (Balanced) |\n",
    "| **Validation** | 955 | ~470 | ~485 | ~50% (Balanced) |\n",
    "| **Test** | **784** | **473** | **311** | **39.7% (Imbalanced)** |\n",
    "\n",
    "> **âš ï¸ Critical Observation:** > The training set is artificially balanced, but the **Test set reflects a more realistic distribution** (Irony is the minority class ~40%).  \n",
    "> Therefore, `Accuracy` is misleading. We prioritize **F1-Score on the Positive Class (Irony)**.\n",
    "\n",
    "**Metrics:**\n",
    "* **Primary:** `F1-Score (Irony)` - Measures the ability to correctly identify irony without false positives.\n",
    "* **Secondary:** `Accuracy` - Global performance indicator.\n",
    "* **Engineering:** `Inference Speed` & `VRAM footprint`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ecce0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anass\\Desktop\\irony-detection-nlp-benchmark\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hardware: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "Precision: torch.bfloat16\n",
      "Device: cuda\n",
      "\n",
      "ðŸ“¥ Chargement du dataset TweetEval (Irony)...\n",
      "Dataset chargÃ© avec succÃ¨s.\n",
      "Train: 2862 | Val: 955 | Test: 784\n",
      "Labels: ['non_irony', 'irony'] (0=Not_Irony, 1=Irony)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb  \n",
    "\n",
    "from datasets import load_dataset\n",
    "from setfit import SetFitModel, Trainer, TrainingArguments\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "from transformers.utils import logging\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "# --- 1. CONFIGURATION HARDWARE (RTX 4060 OPTIMIZATION) ---\n",
    "# On fait taire les warnings non critiques de Transformers\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# La RTX 4060 supporte bfloat16 : c'est le standard actuel pour l'infÃ©rence efficace\n",
    "DTYPE = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "\n",
    "# Seed pour la reproductibilitÃ© (Indispensable pour un benchmark)\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "print(f\"Hardware: {torch.cuda.get_device_name(0) if DEVICE == 'cuda' else 'CPU'}\")\n",
    "print(f\"Precision: {DTYPE}\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "\n",
    "# --- 2. DATA INGESTION ---\n",
    "# Chargement du benchmark officiel \"tweet_eval\" subset \"irony\"\n",
    "print(\"\\nðŸ“¥ Chargement du dataset TweetEval (Irony)...\")\n",
    "dataset = load_dataset(\"tweet_eval\", \"irony\")\n",
    "\n",
    "# Extraction des splits\n",
    "train_ds = dataset[\"train\"]\n",
    "val_ds = dataset[\"validation\"]\n",
    "test_ds = dataset[\"test\"]\n",
    "\n",
    "print(f\"Dataset chargÃ© avec succÃ¨s.\")\n",
    "print(f\"Train: {len(train_ds)} | Val: {len(val_ds)} | Test: {len(test_ds)}\")\n",
    "print(f\"Labels: {train_ds.features['label'].names} (0=Not_Irony, 1=Irony)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2542f85b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Applying preprocessing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2862/2862 [00:00<00:00, 26330.38 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 955/955 [00:00<00:00, 15630.05 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 784/784 [00:00<00:00, 13246.27 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions loaded. Ready to benchmark.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 3. HELPER FUNCTIONS & UTILS ---\n",
    "import time\n",
    "import gc\n",
    "import re\n",
    "\n",
    "# Global leaderboard to accumulate results for the final report\n",
    "leaderboard = []\n",
    "\n",
    "def flush_memory():\n",
    "    \"\"\"\n",
    "    Aggressively cleans up VRAM between models.\n",
    "    Crucial for RTX 4060 (8GB) to avoid OOM (Out Of Memory) errors.\n",
    "    \"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.ipc_collect()\n",
    "    print(\"Memory flushed. VRAM ready for the next round.\")\n",
    "\n",
    "def evaluate_model(y_true, y_pred, model_name, inference_time=None):\n",
    "    \"\"\"\n",
    "    Standardized evaluation function.\n",
    "    Ensures fair comparison across all models using the same metrics.\n",
    "    \"\"\"\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1_irony = f1_score(y_true, y_pred, pos_label=1)\n",
    "    \n",
    "    print(f\"\\nðŸ“Š RESULTS: {model_name}\")\n",
    "    print(f\"  Accuracy: {acc:.2%}\")\n",
    "    print(f\"  F1-Score (Irony): {f1_irony:.2%}\")\n",
    "    if inference_time:\n",
    "        print(f\"  Inference Time: {inference_time:.2f}s\")\n",
    "        \n",
    "    leaderboard.append({\n",
    "        \"Model\": model_name,\n",
    "        \"Accuracy\": acc,\n",
    "        \"F1_Irony\": f1_irony,\n",
    "        \"Inference_Time_s\": inference_time\n",
    "    })\n",
    "    \n",
    "    print(\"-\" * 40)\n",
    "    print(classification_report(y_true, y_pred, target_names=[\"Not_Irony\", \"Irony\"]))\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"\n",
    "    Cardiff models expect @user and http normalization.\n",
    "    \"\"\"\n",
    "    new_text = [\n",
    "    ]\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "# --- APPLY TO ALL SPLITS ---\n",
    "def preprocess_dataset(dataset):\n",
    "    \"\"\"Apply preprocessing to a HuggingFace dataset split.\"\"\"\n",
    "    return dataset.map(lambda x: {\"text\": preprocess(x[\"text\"])})\n",
    "\n",
    "print(\"ðŸ”„ Applying preprocessing...\")\n",
    "\n",
    "train_ds = preprocess_dataset(train_ds)\n",
    "val_ds = preprocess_dataset(val_ds)\n",
    "test_ds = preprocess_dataset(test_ds)\n",
    "\n",
    "print(\"Helper functions loaded. Ready to benchmark.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932075aa",
   "metadata": {},
   "source": [
    "# Cardiff twitter-roberta-base-2021-124m-irony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e04f7354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EXPERIMENT 1: Cardiff RoBERTa (SOTA Reference)\n",
      "============================================================\n",
      "Model: cardiffnlp/twitter-roberta-base-2021-124m-irony\n",
      "Device: cuda\n",
      "âœ… Irony class detected: id=1 â†’ 'irony'\n",
      "Test samples: 784\n",
      "\n",
      "Running inference...\n",
      "âœ… Inference complete in 1.72s\n",
      "   Throughput: 457.1 samples/sec\n",
      "\n",
      "ðŸ“Š RESULTS: Cardiff RoBERTa-2021-124M (SOTA)\n",
      "  Accuracy: 78.57%\n",
      "  F1-Score (Irony): 75.07%\n",
      "----------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Not_Irony       0.86      0.77      0.81       473\n",
      "       Irony       0.70      0.81      0.75       311\n",
      "\n",
      "    accuracy                           0.79       784\n",
      "   macro avg       0.78      0.79      0.78       784\n",
      "weighted avg       0.80      0.79      0.79       784\n",
      "\n",
      "========================================\n",
      "Memory flushed. VRAM ready for the next round.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# EXPERIMENT 1: SOTA Reference â€” Cardiff RoBERTa (2021-124M)\n",
    "# =============================================================================\n",
    "# This is the state-of-the-art industrializable model for irony detection.\n",
    "# Pre-trained on 124M tweets, then fine-tuned on TweetEval Irony.\n",
    "# =============================================================================\n",
    "\n",
    "MODEL_NAME_CARDIFF = \"cardiffnlp/twitter-roberta-base-2021-124m-irony\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EXPERIMENT 1: Cardiff RoBERTa (SOTA Reference)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Model: {MODEL_NAME_CARDIFF}\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "\n",
    "# --- Load Model & Tokenizer ---\n",
    "tokenizer_cardiff = AutoTokenizer.from_pretrained(MODEL_NAME_CARDIFF)\n",
    "model_cardiff = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME_CARDIFF)\n",
    "model_cardiff = model_cardiff.to(DEVICE).eval()\n",
    "\n",
    "# --- Identify Irony class ID (robust method) ---\n",
    "irony_id = 1  # default\n",
    "for k, v in model_cardiff.config.id2label.items():\n",
    "    label = str(v).lower()\n",
    "    if \"irony\" in label and \"non\" not in label and \"not\" not in label:\n",
    "        irony_id = int(k)\n",
    "        print(f\"âœ… Irony class detected: id={irony_id} â†’ '{v}'\")\n",
    "\n",
    "# --- Prepare Test Data ---\n",
    "test_texts = test_ds[\"text\"]\n",
    "test_labels = np.array(test_ds[\"label\"], dtype=np.int64)\n",
    "\n",
    "print(f\"Test samples: {len(test_texts)}\")\n",
    "\n",
    "# --- Inference with timing ---\n",
    "BATCH_SIZE = 64\n",
    "probas = []\n",
    "\n",
    "print(\"\\nRunning inference...\")\n",
    "start_time = time.time()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for i in range(0, len(test_texts), BATCH_SIZE):\n",
    "        batch = test_texts[i:i + BATCH_SIZE]\n",
    "        \n",
    "        # Tokenize\n",
    "        encoded = tokenizer_cardiff(\n",
    "            batch,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=128\n",
    "        ).to(DEVICE)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model_cardiff(**encoded).logits\n",
    "        probs = torch.softmax(logits, dim=-1)[:, irony_id]\n",
    "        probas.append(probs.cpu().numpy())\n",
    "\n",
    "inference_time = time.time() - start_time\n",
    "\n",
    "# --- Predictions ---\n",
    "probas = np.concatenate(probas)\n",
    "y_pred_cardiff = (probas >= 0.5).astype(np.int64)\n",
    "\n",
    "print(f\"âœ… Inference complete in {inference_time:.2f}s\")\n",
    "print(f\"   Throughput: {len(test_texts) / inference_time:.1f} samples/sec\")\n",
    "\n",
    "# --- Evaluation (uses helper function) ---\n",
    "evaluate_model(test_labels, y_pred_cardiff, \"Cardiff RoBERTa-2021-124M (SOTA)\")\n",
    "\n",
    "# --- Cleanup ---\n",
    "del model_cardiff, tokenizer_cardiff\n",
    "flush_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e06c3ce",
   "metadata": {},
   "source": [
    "# Cardiff twitter-roberta-base-irony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75882b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ðŸ“Š EXPERIMENT 1b: Cardiff RoBERTa (Base Version)\n",
      "============================================================\n",
      "Model: cardiffnlp/twitter-roberta-base-irony\n",
      "Device: cuda\n",
      "Test samples: 784\n",
      "\n",
      "ðŸš€ Running inference...\n",
      "âœ… Inference complete in 9.54s\n",
      "   Throughput: 82.2 samples/sec\n",
      "\n",
      "ðŸ“Š RESULTS: Cardiff RoBERTa-Base (Older)\n",
      "  Accuracy: 73.47%\n",
      "  F1-Score (Irony): 62.72%\n",
      "----------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Not_Irony       0.75      0.85      0.79       473\n",
      "       Irony       0.71      0.56      0.63       311\n",
      "\n",
      "    accuracy                           0.73       784\n",
      "   macro avg       0.73      0.71      0.71       784\n",
      "weighted avg       0.73      0.73      0.73       784\n",
      "\n",
      "========================================\n",
      "Memory flushed. VRAM ready for the next round.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# EXPERIMENT 1b: Cardiff RoBERTa (Base Version) â€” Comparison\n",
    "# =============================================================================\n",
    "# Older model, fine-tuned on less data than the 2021-124M version.\n",
    "# Useful to see the impact of pre-training corpus size.\n",
    "# =============================================================================\n",
    "\n",
    "MODEL_NAME_CARDIFF_BASE = \"cardiffnlp/twitter-roberta-base-irony\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EXPERIMENT 1b: Cardiff RoBERTa (Base Version)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Model: {MODEL_NAME_CARDIFF_BASE}\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "\n",
    "# --- Load Pipeline (simpler approach for this model) ---\n",
    "pipe_cardiff_base = pipeline(\n",
    "    \"text-classification\",\n",
    "    model=MODEL_NAME_CARDIFF_BASE,\n",
    "    tokenizer=MODEL_NAME_CARDIFF_BASE,\n",
    "    device=0 if DEVICE == \"cuda\" else -1\n",
    ")\n",
    "\n",
    "# --- Prepare Test Data (already preprocessed) ---\n",
    "test_texts = test_ds[\"text\"]\n",
    "test_labels = np.array(test_ds[\"label\"], dtype=np.int64)\n",
    "\n",
    "print(f\"Test samples: {len(test_texts)}\")\n",
    "\n",
    "# --- Inference with timing ---\n",
    "BATCH_SIZE = 32\n",
    "predictions = []\n",
    "\n",
    "print(\"\\nðŸš€ Running inference...\")\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(0, len(test_texts), BATCH_SIZE):\n",
    "    batch = test_texts[i:i + BATCH_SIZE]\n",
    "    results = pipe_cardiff_base(batch)\n",
    "    \n",
    "    for res in results:\n",
    "        # Map text labels to integers\n",
    "        # 'irony' -> 1, 'non_irony' -> 0\n",
    "        label_str = res['label'].lower()\n",
    "        label_int = 1 if \"irony\" in label_str and \"non\" not in label_str else 0\n",
    "        predictions.append(label_int)\n",
    "\n",
    "inference_time = time.time() - start_time\n",
    "\n",
    "# --- Convert to array ---\n",
    "y_pred_cardiff_base = np.array(predictions, dtype=np.int64)\n",
    "\n",
    "print(f\"âœ… Inference complete in {inference_time:.2f}s\")\n",
    "print(f\"   Throughput: {len(test_texts) / inference_time:.1f} samples/sec\")\n",
    "\n",
    "# --- Evaluation ---\n",
    "evaluate_model(test_labels, y_pred_cardiff_base, \"Cardiff RoBERTa-Base (Older)\")\n",
    "\n",
    "# --- Cleanup ---\n",
    "del pipe_cardiff_base\n",
    "flush_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efeb1463",
   "metadata": {},
   "source": [
    "### Comparative Analysis: Scale & Time Matter\n",
    "\n",
    "**Correction on Model Specs:**\n",
    "We are comparing two very different beasts. The performance gap is driven by two massive factors:\n",
    "\n",
    "1.  **Data Volume (The biggest factor):**\n",
    "    * **Legacy Model:** ~58 Million tweets.\n",
    "    * **Modern Model:** ~124 Million tweets (**+113% more data**).\n",
    "    * *Impact:* The model has seen twice as many examples of sarcasm structure, independent of time.\n",
    "\n",
    "2.  **Temporal Freshness (Concept Drift):**\n",
    "    * **Legacy:** Training stops ~2019.\n",
    "    * **Modern:** Training covers up to Dec 2021.\n",
    "    * *Impact:* Crucial for catching irony based on recent events (Covid, Crypto, Elections).\n",
    "\n",
    "**The Verdict:**\n",
    "* **Legacy Model:** F1-Score (Irony) = **62.7%**\n",
    "* **Modern Model:** F1-Score (Irony) = **75.1%**\n",
    "\n",
    "**Analysis:**\n",
    "The **+12 point gain** is a combination of \"More Data\" (better generalization) and \"Newer Data\" (better context). The Legacy model is simply under-trained by modern standards.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7c2e1c",
   "metadata": {},
   "source": [
    "# Fine-Tuning SetFit (MPNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "363ba932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EXPERIMENT 2: SetFit (MPNet) â€” Challenger\n",
      "============================================================\n",
      "Loading RAW dataset (no preprocessing for SetFit)...\n",
      "Model: sentence-transformers/paraphrase-mpnet-base-v2\n",
      "Device: cuda\n",
      "Train: 2862 | Val: 955 | Test: 784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num unique pairs = 143100\n",
      "  Batch size = 16\n",
      "  Num epochs = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.4499, 'grad_norm': 2.9424524307250977, 'learning_rate': 2.2346368715083798e-08, 'epoch': 0.00011180679785330948}\n",
      "{'embedding_loss': 0.3386, 'grad_norm': 2.0950355529785156, 'learning_rate': 1.11731843575419e-06, 'epoch': 0.005590339892665474}\n",
      "{'embedding_loss': 0.2736, 'grad_norm': 1.3392945528030396, 'learning_rate': 2.23463687150838e-06, 'epoch': 0.011180679785330949}\n",
      "{'embedding_loss': 0.258, 'grad_norm': 0.949404776096344, 'learning_rate': 3.35195530726257e-06, 'epoch': 0.01677101967799642}\n",
      "{'embedding_loss': 0.2547, 'grad_norm': 1.3016060590744019, 'learning_rate': 4.46927374301676e-06, 'epoch': 0.022361359570661897}\n",
      "{'embedding_loss': 0.2571, 'grad_norm': 1.3423534631729126, 'learning_rate': 5.58659217877095e-06, 'epoch': 0.02795169946332737}\n",
      "{'embedding_loss': 0.2533, 'grad_norm': 0.9251423478126526, 'learning_rate': 6.70391061452514e-06, 'epoch': 0.03354203935599284}\n",
      "{'embedding_loss': 0.2553, 'grad_norm': 0.9265353083610535, 'learning_rate': 7.82122905027933e-06, 'epoch': 0.03913237924865832}\n",
      "{'embedding_loss': 0.2531, 'grad_norm': 1.0270229578018188, 'learning_rate': 8.93854748603352e-06, 'epoch': 0.044722719141323794}\n",
      "{'embedding_loss': 0.2582, 'grad_norm': 0.9499971866607666, 'learning_rate': 1.005586592178771e-05, 'epoch': 0.05031305903398927}\n",
      "{'embedding_loss': 0.2583, 'grad_norm': 0.9831147193908691, 'learning_rate': 1.11731843575419e-05, 'epoch': 0.05590339892665474}\n",
      "{'embedding_loss': 0.2536, 'grad_norm': 1.2845978736877441, 'learning_rate': 1.229050279329609e-05, 'epoch': 0.061493738819320215}\n",
      "{'embedding_loss': 0.249, 'grad_norm': 2.2536122798919678, 'learning_rate': 1.340782122905028e-05, 'epoch': 0.06708407871198568}\n",
      "{'embedding_loss': 0.2471, 'grad_norm': 1.2521640062332153, 'learning_rate': 1.452513966480447e-05, 'epoch': 0.07267441860465117}\n",
      "{'embedding_loss': 0.2503, 'grad_norm': 1.3838599920272827, 'learning_rate': 1.564245810055866e-05, 'epoch': 0.07826475849731664}\n",
      "{'embedding_loss': 0.2387, 'grad_norm': 1.5574820041656494, 'learning_rate': 1.6759776536312852e-05, 'epoch': 0.0838550983899821}\n",
      "{'embedding_loss': 0.2373, 'grad_norm': 2.2698535919189453, 'learning_rate': 1.787709497206704e-05, 'epoch': 0.08944543828264759}\n",
      "{'embedding_loss': 0.2291, 'grad_norm': 1.7276465892791748, 'learning_rate': 1.8994413407821232e-05, 'epoch': 0.09503577817531306}\n",
      "{'embedding_loss': 0.23, 'grad_norm': 1.3114122152328491, 'learning_rate': 1.9987576096409493e-05, 'epoch': 0.10062611806797854}\n",
      "{'embedding_loss': 0.2109, 'grad_norm': 2.3414506912231445, 'learning_rate': 1.9863337060504412e-05, 'epoch': 0.10621645796064401}\n",
      "{'embedding_loss': 0.2114, 'grad_norm': 2.6234512329101562, 'learning_rate': 1.973909802459933e-05, 'epoch': 0.11180679785330948}\n",
      "{'embedding_loss': 0.2011, 'grad_norm': 2.624570369720459, 'learning_rate': 1.961485898869425e-05, 'epoch': 0.11739713774597496}\n",
      "{'embedding_loss': 0.1928, 'grad_norm': 3.219536781311035, 'learning_rate': 1.9490619952789166e-05, 'epoch': 0.12298747763864043}\n",
      "{'embedding_loss': 0.1667, 'grad_norm': 2.8485770225524902, 'learning_rate': 1.9366380916884085e-05, 'epoch': 0.1285778175313059}\n",
      "{'embedding_loss': 0.1514, 'grad_norm': 2.70379376411438, 'learning_rate': 1.9242141880979008e-05, 'epoch': 0.13416815742397137}\n",
      "{'embedding_loss': 0.1489, 'grad_norm': 2.472437858581543, 'learning_rate': 1.9117902845073924e-05, 'epoch': 0.13975849731663686}\n",
      "{'embedding_loss': 0.1269, 'grad_norm': 2.722996950149536, 'learning_rate': 1.8993663809168843e-05, 'epoch': 0.14534883720930233}\n",
      "{'embedding_loss': 0.1128, 'grad_norm': 2.9631190299987793, 'learning_rate': 1.8869424773263762e-05, 'epoch': 0.1509391771019678}\n",
      "{'embedding_loss': 0.0927, 'grad_norm': 1.5454744100570679, 'learning_rate': 1.874518573735868e-05, 'epoch': 0.15652951699463327}\n",
      "{'embedding_loss': 0.0915, 'grad_norm': 3.134796380996704, 'learning_rate': 1.8620946701453597e-05, 'epoch': 0.16211985688729874}\n",
      "{'embedding_loss': 0.0968, 'grad_norm': 6.773072719573975, 'learning_rate': 1.8496707665548516e-05, 'epoch': 0.1677101967799642}\n",
      "{'embedding_loss': 0.0751, 'grad_norm': 13.863036155700684, 'learning_rate': 1.8372468629643435e-05, 'epoch': 0.1733005366726297}\n",
      "{'embedding_loss': 0.0579, 'grad_norm': 12.287200927734375, 'learning_rate': 1.8248229593738355e-05, 'epoch': 0.17889087656529518}\n",
      "{'embedding_loss': 0.0527, 'grad_norm': 1.910906195640564, 'learning_rate': 1.8123990557833274e-05, 'epoch': 0.18448121645796065}\n",
      "{'embedding_loss': 0.0514, 'grad_norm': 1.1753309965133667, 'learning_rate': 1.799975152192819e-05, 'epoch': 0.19007155635062611}\n",
      "{'embedding_loss': 0.0487, 'grad_norm': 8.770803451538086, 'learning_rate': 1.787551248602311e-05, 'epoch': 0.19566189624329158}\n",
      "{'embedding_loss': 0.0457, 'grad_norm': 4.542657852172852, 'learning_rate': 1.7751273450118028e-05, 'epoch': 0.20125223613595708}\n",
      "{'embedding_loss': 0.0375, 'grad_norm': 1.0599693059921265, 'learning_rate': 1.7627034414212947e-05, 'epoch': 0.20684257602862255}\n",
      "{'embedding_loss': 0.034, 'grad_norm': 0.023553187027573586, 'learning_rate': 1.7502795378307866e-05, 'epoch': 0.21243291592128802}\n",
      "{'embedding_loss': 0.0347, 'grad_norm': 2.292282819747925, 'learning_rate': 1.7378556342402782e-05, 'epoch': 0.2180232558139535}\n",
      "{'embedding_loss': 0.0321, 'grad_norm': 0.04518797621130943, 'learning_rate': 1.7254317306497705e-05, 'epoch': 0.22361359570661896}\n",
      "{'embedding_loss': 0.0392, 'grad_norm': 4.086015701293945, 'learning_rate': 1.713007827059262e-05, 'epoch': 0.22920393559928443}\n",
      "{'embedding_loss': 0.0326, 'grad_norm': 0.03808930143713951, 'learning_rate': 1.700583923468754e-05, 'epoch': 0.23479427549194992}\n",
      "{'embedding_loss': 0.0164, 'grad_norm': 0.5754238367080688, 'learning_rate': 1.688160019878246e-05, 'epoch': 0.2403846153846154}\n",
      "{'embedding_loss': 0.0192, 'grad_norm': 0.4135614335536957, 'learning_rate': 1.6757361162877378e-05, 'epoch': 0.24597495527728086}\n",
      "{'embedding_loss': 0.0173, 'grad_norm': 0.18837366998195648, 'learning_rate': 1.6633122126972297e-05, 'epoch': 0.25156529516994636}\n",
      "{'embedding_loss': 0.0244, 'grad_norm': 0.17353267967700958, 'learning_rate': 1.6508883091067213e-05, 'epoch': 0.2571556350626118}\n",
      "{'embedding_loss': 0.0094, 'grad_norm': 0.02480807714164257, 'learning_rate': 1.6384644055162132e-05, 'epoch': 0.2627459749552773}\n",
      "{'embedding_loss': 0.0135, 'grad_norm': 0.034190960228443146, 'learning_rate': 1.626040501925705e-05, 'epoch': 0.26833631484794274}\n",
      "{'embedding_loss': 0.0176, 'grad_norm': 0.5139182209968567, 'learning_rate': 1.613616598335197e-05, 'epoch': 0.27392665474060823}\n",
      "{'embedding_loss': 0.0191, 'grad_norm': 0.27435219287872314, 'learning_rate': 1.601192694744689e-05, 'epoch': 0.27951699463327373}\n",
      "{'embedding_loss': 0.0124, 'grad_norm': 0.007625843398272991, 'learning_rate': 1.5887687911541806e-05, 'epoch': 0.28510733452593917}\n",
      "{'embedding_loss': 0.0092, 'grad_norm': 0.38361629843711853, 'learning_rate': 1.5763448875636725e-05, 'epoch': 0.29069767441860467}\n",
      "{'embedding_loss': 0.0034, 'grad_norm': 11.945673942565918, 'learning_rate': 1.5639209839731644e-05, 'epoch': 0.2962880143112701}\n",
      "{'embedding_loss': 0.0147, 'grad_norm': 0.24974435567855835, 'learning_rate': 1.5514970803826563e-05, 'epoch': 0.3018783542039356}\n",
      "{'embedding_loss': 0.0061, 'grad_norm': 3.9502339363098145, 'learning_rate': 1.5390731767921483e-05, 'epoch': 0.30746869409660105}\n",
      "{'embedding_loss': 0.0086, 'grad_norm': 0.004748556762933731, 'learning_rate': 1.5266492732016402e-05, 'epoch': 0.31305903398926654}\n",
      "{'embedding_loss': 0.0074, 'grad_norm': 0.9327272772789001, 'learning_rate': 1.514225369611132e-05, 'epoch': 0.31864937388193204}\n",
      "{'embedding_loss': 0.0046, 'grad_norm': 0.027330590412020683, 'learning_rate': 1.5018014660206237e-05, 'epoch': 0.3242397137745975}\n",
      "{'embedding_loss': 0.0099, 'grad_norm': 0.05200140178203583, 'learning_rate': 1.4893775624301156e-05, 'epoch': 0.329830053667263}\n",
      "{'embedding_loss': 0.0065, 'grad_norm': 0.02864564210176468, 'learning_rate': 1.4769536588396077e-05, 'epoch': 0.3354203935599284}\n",
      "{'embedding_loss': 0.0093, 'grad_norm': 0.004492404405027628, 'learning_rate': 1.4645297552490994e-05, 'epoch': 0.3410107334525939}\n",
      "{'embedding_loss': 0.0061, 'grad_norm': 0.22316758334636688, 'learning_rate': 1.4521058516585913e-05, 'epoch': 0.3466010733452594}\n",
      "{'embedding_loss': 0.0115, 'grad_norm': 0.026335040107369423, 'learning_rate': 1.4396819480680831e-05, 'epoch': 0.35219141323792486}\n",
      "{'embedding_loss': 0.0023, 'grad_norm': 2.6751253604888916, 'learning_rate': 1.427258044477575e-05, 'epoch': 0.35778175313059035}\n",
      "{'embedding_loss': 0.01, 'grad_norm': 0.014340069144964218, 'learning_rate': 1.414834140887067e-05, 'epoch': 0.3633720930232558}\n",
      "{'embedding_loss': 0.0026, 'grad_norm': 7.185510158538818, 'learning_rate': 1.4024102372965587e-05, 'epoch': 0.3689624329159213}\n",
      "{'embedding_loss': 0.0095, 'grad_norm': 4.791018486022949, 'learning_rate': 1.3899863337060506e-05, 'epoch': 0.3745527728085868}\n",
      "{'embedding_loss': 0.0104, 'grad_norm': 1.5106309652328491, 'learning_rate': 1.3775624301155424e-05, 'epoch': 0.38014311270125223}\n",
      "{'embedding_loss': 0.0095, 'grad_norm': 0.015152661129832268, 'learning_rate': 1.3651385265250343e-05, 'epoch': 0.3857334525939177}\n",
      "{'embedding_loss': 0.0077, 'grad_norm': 1.9780551195144653, 'learning_rate': 1.352714622934526e-05, 'epoch': 0.39132379248658317}\n",
      "{'embedding_loss': 0.009, 'grad_norm': 0.035393133759498596, 'learning_rate': 1.340290719344018e-05, 'epoch': 0.39691413237924866}\n",
      "{'embedding_loss': 0.0117, 'grad_norm': 0.008419088087975979, 'learning_rate': 1.3278668157535099e-05, 'epoch': 0.40250447227191416}\n",
      "{'embedding_loss': 0.0027, 'grad_norm': 0.03477681800723076, 'learning_rate': 1.3154429121630016e-05, 'epoch': 0.4080948121645796}\n",
      "{'embedding_loss': 0.0119, 'grad_norm': 0.04410418123006821, 'learning_rate': 1.3030190085724937e-05, 'epoch': 0.4136851520572451}\n",
      "{'embedding_loss': 0.0067, 'grad_norm': 3.676083564758301, 'learning_rate': 1.2905951049819853e-05, 'epoch': 0.41927549194991054}\n",
      "{'embedding_loss': 0.0063, 'grad_norm': 1.3273532390594482, 'learning_rate': 1.2781712013914774e-05, 'epoch': 0.42486583184257604}\n",
      "{'embedding_loss': 0.0083, 'grad_norm': 0.03426027670502663, 'learning_rate': 1.2657472978009693e-05, 'epoch': 0.4304561717352415}\n",
      "{'embedding_loss': 0.0095, 'grad_norm': 0.03760749101638794, 'learning_rate': 1.253323394210461e-05, 'epoch': 0.436046511627907}\n",
      "{'embedding_loss': 0.0065, 'grad_norm': 2.13566517829895, 'learning_rate': 1.240899490619953e-05, 'epoch': 0.44163685152057247}\n",
      "{'embedding_loss': 0.006, 'grad_norm': 0.07093972712755203, 'learning_rate': 1.2284755870294447e-05, 'epoch': 0.4472271914132379}\n",
      "{'embedding_loss': 0.0034, 'grad_norm': 0.0050281621515750885, 'learning_rate': 1.2160516834389366e-05, 'epoch': 0.4528175313059034}\n",
      "{'embedding_loss': 0.0064, 'grad_norm': 0.06680307537317276, 'learning_rate': 1.2036277798484284e-05, 'epoch': 0.45840787119856885}\n",
      "{'embedding_loss': 0.0063, 'grad_norm': 0.023304471746087074, 'learning_rate': 1.1912038762579203e-05, 'epoch': 0.46399821109123435}\n",
      "{'embedding_loss': 0.0081, 'grad_norm': 0.010003121569752693, 'learning_rate': 1.1787799726674122e-05, 'epoch': 0.46958855098389984}\n",
      "{'embedding_loss': 0.005, 'grad_norm': 0.06711642444133759, 'learning_rate': 1.166356069076904e-05, 'epoch': 0.4751788908765653}\n",
      "{'embedding_loss': 0.0032, 'grad_norm': 0.021363668143749237, 'learning_rate': 1.1539321654863959e-05, 'epoch': 0.4807692307692308}\n",
      "{'embedding_loss': 0.0059, 'grad_norm': 0.03839075565338135, 'learning_rate': 1.1415082618958876e-05, 'epoch': 0.4863595706618962}\n",
      "{'embedding_loss': 0.0004, 'grad_norm': 0.007349743973463774, 'learning_rate': 1.1290843583053797e-05, 'epoch': 0.4919499105545617}\n",
      "{'embedding_loss': 0.0037, 'grad_norm': 0.004691369365900755, 'learning_rate': 1.1166604547148717e-05, 'epoch': 0.4975402504472272}\n",
      "{'embedding_loss': 0.0021, 'grad_norm': 0.018609866499900818, 'learning_rate': 1.1042365511243634e-05, 'epoch': 0.5031305903398927}\n",
      "{'embedding_loss': 0.0025, 'grad_norm': 0.009777036495506763, 'learning_rate': 1.0918126475338553e-05, 'epoch': 0.5087209302325582}\n",
      "{'embedding_loss': 0.0026, 'grad_norm': 0.020849745720624924, 'learning_rate': 1.079388743943347e-05, 'epoch': 0.5143112701252236}\n",
      "{'embedding_loss': 0.0066, 'grad_norm': 0.5831356048583984, 'learning_rate': 1.066964840352839e-05, 'epoch': 0.519901610017889}\n",
      "{'embedding_loss': 0.0055, 'grad_norm': 0.018756546080112457, 'learning_rate': 1.0545409367623307e-05, 'epoch': 0.5254919499105546}\n",
      "{'embedding_loss': 0.0051, 'grad_norm': 0.01847134530544281, 'learning_rate': 1.0421170331718227e-05, 'epoch': 0.53108228980322}\n",
      "{'embedding_loss': 0.0001, 'grad_norm': 0.012775309383869171, 'learning_rate': 1.0296931295813146e-05, 'epoch': 0.5366726296958855}\n",
      "{'embedding_loss': 0.0037, 'grad_norm': 0.007523198612034321, 'learning_rate': 1.0172692259908063e-05, 'epoch': 0.542262969588551}\n",
      "{'embedding_loss': 0.0009, 'grad_norm': 0.0035960162058472633, 'learning_rate': 1.0048453224002983e-05, 'epoch': 0.5478533094812165}\n",
      "{'embedding_loss': 0.0002, 'grad_norm': 0.002259972970932722, 'learning_rate': 9.924214188097902e-06, 'epoch': 0.5534436493738819}\n",
      "{'embedding_loss': 0.0027, 'grad_norm': 0.0055044833570718765, 'learning_rate': 9.79997515219282e-06, 'epoch': 0.5590339892665475}\n",
      "{'embedding_loss': 0.004, 'grad_norm': 0.003300379728898406, 'learning_rate': 9.675736116287738e-06, 'epoch': 0.5646243291592129}\n",
      "{'embedding_loss': 0.0041, 'grad_norm': 0.010249093174934387, 'learning_rate': 9.551497080382656e-06, 'epoch': 0.5702146690518783}\n",
      "{'embedding_loss': 0.0015, 'grad_norm': 0.0021213823929429054, 'learning_rate': 9.427258044477575e-06, 'epoch': 0.5758050089445438}\n",
      "{'embedding_loss': 0.0026, 'grad_norm': 0.004847007803618908, 'learning_rate': 9.303019008572494e-06, 'epoch': 0.5813953488372093}\n",
      "{'embedding_loss': 0.0025, 'grad_norm': 0.0025378919672220945, 'learning_rate': 9.178779972667414e-06, 'epoch': 0.5869856887298748}\n",
      "{'embedding_loss': 0.0002, 'grad_norm': 0.0023383344523608685, 'learning_rate': 9.054540936762331e-06, 'epoch': 0.5925760286225402}\n",
      "{'embedding_loss': 0.0037, 'grad_norm': 0.015990393236279488, 'learning_rate': 8.93030190085725e-06, 'epoch': 0.5981663685152058}\n",
      "{'embedding_loss': 0.0051, 'grad_norm': 12.108575820922852, 'learning_rate': 8.806062864952168e-06, 'epoch': 0.6037567084078712}\n",
      "{'embedding_loss': 0.0051, 'grad_norm': 0.030183136463165283, 'learning_rate': 8.681823829047087e-06, 'epoch': 0.6093470483005367}\n",
      "{'embedding_loss': 0.0047, 'grad_norm': 0.010634505189955235, 'learning_rate': 8.557584793142006e-06, 'epoch': 0.6149373881932021}\n",
      "{'embedding_loss': 0.0031, 'grad_norm': 0.01647491380572319, 'learning_rate': 8.433345757236925e-06, 'epoch': 0.6205277280858676}\n",
      "{'embedding_loss': 0.0024, 'grad_norm': 0.001833824091590941, 'learning_rate': 8.309106721331843e-06, 'epoch': 0.6261180679785331}\n",
      "{'embedding_loss': 0.0004, 'grad_norm': 0.0012488258071243763, 'learning_rate': 8.184867685426762e-06, 'epoch': 0.6317084078711985}\n",
      "{'embedding_loss': 0.0045, 'grad_norm': 0.013086172752082348, 'learning_rate': 8.06062864952168e-06, 'epoch': 0.6372987477638641}\n",
      "{'embedding_loss': 0.0065, 'grad_norm': 0.01861410215497017, 'learning_rate': 7.936389613616599e-06, 'epoch': 0.6428890876565295}\n",
      "{'embedding_loss': 0.0045, 'grad_norm': 0.0012322432594373822, 'learning_rate': 7.812150577711518e-06, 'epoch': 0.648479427549195}\n",
      "{'embedding_loss': 0.0064, 'grad_norm': 0.023150624707341194, 'learning_rate': 7.687911541806437e-06, 'epoch': 0.6540697674418605}\n",
      "{'embedding_loss': 0.0015, 'grad_norm': 0.006267883814871311, 'learning_rate': 7.563672505901355e-06, 'epoch': 0.659660107334526}\n",
      "{'embedding_loss': 0.0001, 'grad_norm': 0.003840255318209529, 'learning_rate': 7.439433469996273e-06, 'epoch': 0.6652504472271914}\n",
      "{'embedding_loss': 0.0024, 'grad_norm': 0.014996900223195553, 'learning_rate': 7.315194434091192e-06, 'epoch': 0.6708407871198568}\n",
      "{'embedding_loss': 0.0038, 'grad_norm': 0.007371790707111359, 'learning_rate': 7.1909553981861105e-06, 'epoch': 0.6764311270125224}\n",
      "{'embedding_loss': 0.0013, 'grad_norm': 1.718602180480957, 'learning_rate': 7.06671636228103e-06, 'epoch': 0.6820214669051878}\n",
      "{'embedding_loss': 0.0041, 'grad_norm': 0.033388279378414154, 'learning_rate': 6.942477326375948e-06, 'epoch': 0.6876118067978533}\n",
      "{'embedding_loss': 0.0013, 'grad_norm': 0.0038405770901590586, 'learning_rate': 6.8182382904708664e-06, 'epoch': 0.6932021466905188}\n",
      "{'embedding_loss': 0.0013, 'grad_norm': 0.39082860946655273, 'learning_rate': 6.693999254565785e-06, 'epoch': 0.6987924865831843}\n",
      "{'embedding_loss': 0.003, 'grad_norm': 0.010667255148291588, 'learning_rate': 6.569760218660703e-06, 'epoch': 0.7043828264758497}\n",
      "{'embedding_loss': 0.0036, 'grad_norm': 0.008703868836164474, 'learning_rate': 6.4455211827556215e-06, 'epoch': 0.7099731663685152}\n",
      "{'embedding_loss': 0.0005, 'grad_norm': 0.004958686884492636, 'learning_rate': 6.3212821468505415e-06, 'epoch': 0.7155635062611807}\n",
      "{'embedding_loss': 0.0016, 'grad_norm': 0.020222783088684082, 'learning_rate': 6.19704311094546e-06, 'epoch': 0.7211538461538461}\n",
      "{'embedding_loss': 0.0009, 'grad_norm': 0.004062817897647619, 'learning_rate': 6.072804075040378e-06, 'epoch': 0.7267441860465116}\n",
      "{'embedding_loss': 0.0025, 'grad_norm': 0.17342017590999603, 'learning_rate': 5.948565039135297e-06, 'epoch': 0.7323345259391771}\n",
      "{'embedding_loss': 0.0001, 'grad_norm': 0.003380973357707262, 'learning_rate': 5.824326003230215e-06, 'epoch': 0.7379248658318426}\n",
      "{'embedding_loss': 0.0013, 'grad_norm': 0.002182604046538472, 'learning_rate': 5.700086967325133e-06, 'epoch': 0.743515205724508}\n",
      "{'embedding_loss': 0.0025, 'grad_norm': 0.008024645037949085, 'learning_rate': 5.575847931420053e-06, 'epoch': 0.7491055456171736}\n",
      "{'embedding_loss': 0.0012, 'grad_norm': 0.015187946148216724, 'learning_rate': 5.451608895514972e-06, 'epoch': 0.754695885509839}\n",
      "{'embedding_loss': 0.0013, 'grad_norm': 0.0006475764093920588, 'learning_rate': 5.32736985960989e-06, 'epoch': 0.7602862254025045}\n",
      "{'embedding_loss': 0.0001, 'grad_norm': 0.000658307399135083, 'learning_rate': 5.203130823704808e-06, 'epoch': 0.7658765652951699}\n",
      "{'embedding_loss': 0.0015, 'grad_norm': 0.0017087443266063929, 'learning_rate': 5.078891787799727e-06, 'epoch': 0.7714669051878354}\n",
      "{'embedding_loss': 0.0012, 'grad_norm': 0.18225502967834473, 'learning_rate': 4.954652751894646e-06, 'epoch': 0.7770572450805009}\n",
      "{'embedding_loss': 0.0012, 'grad_norm': 0.0035652031656354666, 'learning_rate': 4.830413715989564e-06, 'epoch': 0.7826475849731663}\n",
      "{'embedding_loss': 0.0012, 'grad_norm': 0.0013572204625234008, 'learning_rate': 4.7061746800844835e-06, 'epoch': 0.7882379248658319}\n",
      "{'embedding_loss': 0.0007, 'grad_norm': 0.0006672432646155357, 'learning_rate': 4.581935644179402e-06, 'epoch': 0.7938282647584973}\n",
      "{'embedding_loss': 0.0013, 'grad_norm': 0.017805427312850952, 'learning_rate': 4.45769660827432e-06, 'epoch': 0.7994186046511628}\n",
      "{'embedding_loss': 0.0001, 'grad_norm': 0.0018398963147774339, 'learning_rate': 4.333457572369239e-06, 'epoch': 0.8050089445438283}\n",
      "{'embedding_loss': 0.0011, 'grad_norm': 0.002471145009621978, 'learning_rate': 4.209218536464158e-06, 'epoch': 0.8105992844364938}\n",
      "{'embedding_loss': 0.0031, 'grad_norm': 0.011997437104582787, 'learning_rate': 4.084979500559076e-06, 'epoch': 0.8161896243291592}\n",
      "{'embedding_loss': 0.0001, 'grad_norm': 0.007041081320494413, 'learning_rate': 3.960740464653995e-06, 'epoch': 0.8217799642218246}\n",
      "{'embedding_loss': 0.0032, 'grad_norm': 0.005859544035047293, 'learning_rate': 3.836501428748914e-06, 'epoch': 0.8273703041144902}\n",
      "{'embedding_loss': 0.0017, 'grad_norm': 0.00855919998139143, 'learning_rate': 3.7122623928438316e-06, 'epoch': 0.8329606440071556}\n",
      "{'embedding_loss': 0.0003, 'grad_norm': 0.002061019418761134, 'learning_rate': 3.5880233569387507e-06, 'epoch': 0.8385509838998211}\n",
      "{'embedding_loss': 0.0013, 'grad_norm': 0.0035408656112849712, 'learning_rate': 3.463784321033669e-06, 'epoch': 0.8441413237924866}\n",
      "{'embedding_loss': 0.0001, 'grad_norm': 0.004285246133804321, 'learning_rate': 3.3395452851285874e-06, 'epoch': 0.8497316636851521}\n",
      "{'embedding_loss': 0.0012, 'grad_norm': 0.006902510300278664, 'learning_rate': 3.2153062492235066e-06, 'epoch': 0.8553220035778175}\n",
      "{'embedding_loss': 0.0001, 'grad_norm': 0.0038483457174152136, 'learning_rate': 3.091067213318425e-06, 'epoch': 0.860912343470483}\n",
      "{'embedding_loss': 0.0001, 'grad_norm': 0.0027514684479683638, 'learning_rate': 2.9668281774133433e-06, 'epoch': 0.8665026833631485}\n",
      "{'embedding_loss': 0.0, 'grad_norm': 0.002243935829028487, 'learning_rate': 2.8425891415082625e-06, 'epoch': 0.872093023255814}\n",
      "{'embedding_loss': 0.0, 'grad_norm': 0.0021345980931073427, 'learning_rate': 2.718350105603181e-06, 'epoch': 0.8776833631484794}\n",
      "{'embedding_loss': 0.0001, 'grad_norm': 0.007967817597091198, 'learning_rate': 2.5941110696980992e-06, 'epoch': 0.8832737030411449}\n",
      "{'embedding_loss': 0.0015, 'grad_norm': 0.0031427000649273396, 'learning_rate': 2.469872033793018e-06, 'epoch': 0.8888640429338104}\n",
      "{'embedding_loss': 0.0013, 'grad_norm': 0.2694939374923706, 'learning_rate': 2.345632997887937e-06, 'epoch': 0.8944543828264758}\n",
      "{'embedding_loss': 0.0017, 'grad_norm': 0.005637070629745722, 'learning_rate': 2.221393961982855e-06, 'epoch': 0.9000447227191414}\n",
      "{'embedding_loss': 0.0005, 'grad_norm': 0.001533313887193799, 'learning_rate': 2.097154926077774e-06, 'epoch': 0.9056350626118068}\n",
      "{'embedding_loss': 0.0, 'grad_norm': 0.001901238108985126, 'learning_rate': 1.9729158901726923e-06, 'epoch': 0.9112254025044723}\n",
      "{'embedding_loss': 0.0, 'grad_norm': 0.025181962177157402, 'learning_rate': 1.8486768542676108e-06, 'epoch': 0.9168157423971377}\n",
      "{'embedding_loss': 0.0, 'grad_norm': 0.0014611686347052455, 'learning_rate': 1.7244378183625296e-06, 'epoch': 0.9224060822898033}\n",
      "{'embedding_loss': 0.001, 'grad_norm': 0.0024479313287883997, 'learning_rate': 1.6001987824574484e-06, 'epoch': 0.9279964221824687}\n",
      "{'embedding_loss': 0.0021, 'grad_norm': 0.002157737733796239, 'learning_rate': 1.475959746552367e-06, 'epoch': 0.9335867620751341}\n",
      "{'embedding_loss': 0.0, 'grad_norm': 0.0004947552806697786, 'learning_rate': 1.3517207106472855e-06, 'epoch': 0.9391771019677997}\n",
      "{'embedding_loss': 0.0, 'grad_norm': 0.0019502902869135141, 'learning_rate': 1.227481674742204e-06, 'epoch': 0.9447674418604651}\n",
      "{'embedding_loss': 0.0, 'grad_norm': 0.0005996578256599605, 'learning_rate': 1.1032426388371226e-06, 'epoch': 0.9503577817531306}\n",
      "{'embedding_loss': 0.0, 'grad_norm': 0.0019872321281582117, 'learning_rate': 9.790036029320412e-07, 'epoch': 0.955948121645796}\n",
      "{'embedding_loss': 0.0001, 'grad_norm': 0.0004274981329217553, 'learning_rate': 8.5476456702696e-07, 'epoch': 0.9615384615384616}\n",
      "{'embedding_loss': 0.0, 'grad_norm': 0.0027922503650188446, 'learning_rate': 7.305255311218785e-07, 'epoch': 0.967128801431127}\n",
      "{'embedding_loss': 0.0, 'grad_norm': 0.0007642044220119715, 'learning_rate': 6.062864952167972e-07, 'epoch': 0.9727191413237924}\n",
      "{'embedding_loss': 0.0001, 'grad_norm': 0.008744128979742527, 'learning_rate': 4.820474593117159e-07, 'epoch': 0.978309481216458}\n",
      "{'embedding_loss': 0.0, 'grad_norm': 0.0012811280321329832, 'learning_rate': 3.5780842340663436e-07, 'epoch': 0.9838998211091234}\n",
      "{'embedding_loss': 0.0003, 'grad_norm': 0.002403225749731064, 'learning_rate': 2.33569387501553e-07, 'epoch': 0.9894901610017889}\n",
      "{'embedding_loss': 0.0, 'grad_norm': 0.0027611111290752888, 'learning_rate': 1.0933035159647162e-07, 'epoch': 0.9950805008944544}\n",
      "{'eval_embedding_loss': 0.38108664751052856, 'eval_embedding_runtime': 141.8993, 'eval_embedding_samples_per_second': 336.506, 'eval_embedding_steps_per_second': 21.036, 'epoch': 1.0}\n",
      "{'train_runtime': 2053.3279, 'train_samples_per_second': 69.692, 'train_steps_per_second': 4.356, 'train_loss': 0.04202731440926134, 'epoch': 1.0}\n",
      "âœ… Training complete in 2068.9s\n",
      "\n",
      "ðŸš€ Inference on test set...\n",
      "âœ… Inference complete in 0.96s\n",
      "\n",
      "ðŸ“Š RESULTS: SetFit (MPNet)\n",
      "  Accuracy: 74.23%\n",
      "  F1-Score (Irony): 71.79%\n",
      "  Inference Time: 0.96s\n",
      "----------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Not_Irony       0.86      0.69      0.76       473\n",
      "       Irony       0.63      0.83      0.72       311\n",
      "\n",
      "    accuracy                           0.74       784\n",
      "   macro avg       0.75      0.76      0.74       784\n",
      "weighted avg       0.77      0.74      0.75       784\n",
      "\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# EXPERIMENT 2: SetFit (MPNet) â€” Challenger\n",
    "# =============================================================================\n",
    "\n",
    "MODEL_NAME_SETFIT = \"sentence-transformers/paraphrase-mpnet-base-v2\"\n",
    "LABELS_NAMES = [\"NOT_IRONY\", \"IRONY\"]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EXPERIMENT 2: SetFit (MPNet) â€” Challenger\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# --- IMPORTANT: Reload RAW data (no preprocessing for MPNet) ---\n",
    "print(\"Loading RAW dataset (no preprocessing for SetFit)...\")\n",
    "raw_dataset = load_dataset(\"tweet_eval\", \"irony\")\n",
    "train_raw = raw_dataset[\"train\"]\n",
    "val_raw = raw_dataset[\"validation\"]\n",
    "test_raw = raw_dataset[\"test\"]\n",
    "\n",
    "print(f\"Model: {MODEL_NAME_SETFIT}\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Train: {len(train_raw)} | Val: {len(val_raw)} | Test: {len(test_raw)}\")\n",
    "\n",
    "# --- Model ---\n",
    "model_setfit = SetFitModel.from_pretrained(\n",
    "    MODEL_NAME_SETFIT,\n",
    "    device=DEVICE,\n",
    "    labels=LABELS_NAMES,\n",
    ")\n",
    "\n",
    "# --- Training Arguments ---\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"setfit_checkpoints_irony\",\n",
    "    batch_size=16,\n",
    "    num_epochs=1,\n",
    "    num_iterations=25,\n",
    "    body_learning_rate=2e-5,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"embedding_loss\",\n",
    "    greater_is_better=False,\n",
    "    use_amp=(DEVICE == \"cuda\"),\n",
    "    report_to=\"none\",\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "# --- Trainer (with RAW data) ---\n",
    "trainer = Trainer(\n",
    "    model=model_setfit,\n",
    "    args=args,\n",
    "    train_dataset=train_raw,  # RAW\n",
    "    eval_dataset=val_raw,     # RAW\n",
    "    metric=\"accuracy\",\n",
    ")\n",
    "\n",
    "# --- Train ---\n",
    "print(\"\\n Training...\")\n",
    "start_train = time.time()\n",
    "trainer.train()\n",
    "train_time = time.time() - start_train\n",
    "print(f\"âœ… Training complete in {train_time:.1f}s\")\n",
    "\n",
    "\n",
    "# --- Inference ---\n",
    "print(\"\\nðŸš€ Inference on test set...\")\n",
    "test_labels = np.array(test_ds[\"label\"], dtype=np.int64)\n",
    "\n",
    "start_inference = time.time()\n",
    "raw_preds = model_setfit.predict(test_ds[\"text\"])\n",
    "\n",
    "# ðŸ”§ FIX AUTOMATIQUE : Conversion String -> Int si nÃ©cessaire\n",
    "# Si le modÃ¨le renvoie ['NOT_IRONY', 'IRONY'], on mappe vers [0, 1]\n",
    "if len(raw_preds) > 0 and isinstance(raw_preds[0], str):\n",
    "    label_map = {\"NOT_IRONY\": 0, \"IRONY\": 1, \"not_irony\": 0, \"irony\": 1}\n",
    "    y_pred_setfit = np.array([label_map.get(str(p), 0) for p in raw_preds], dtype=np.int64)\n",
    "else:\n",
    "    # Si c'est dÃ©jÃ  des chiffres, on passe direct\n",
    "    y_pred_setfit = np.array(raw_preds, dtype=np.int64)\n",
    "\n",
    "inference_time = time.time() - start_inference\n",
    "print(f\"âœ… Inference complete in {inference_time:.2f}s\")\n",
    "\n",
    "# --- Evaluation ---\n",
    "evaluate_model(test_labels, y_pred_setfit, \"SetFit (MPNet)\", inference_time)\n",
    "\n",
    "# --- Save ---\n",
    "SETFIT_SAVE_PATH = \"models/setfit-mpnet-irony\"\n",
    "# On s'assure que le dossier existe pour Ã©viter une erreur\n",
    "os.makedirs(SETFIT_SAVE_PATH, exist_ok=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17abccb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "ðŸ“Š SetFit Model Parameters\n",
      "==================================================\n",
      "Body type: SentenceTransformer\n",
      "Head type: LogisticRegression\n",
      "--------------------------------------------------\n",
      "Body (MPNet):         109,486,464 params\n",
      "Head (sklearn):               769 params\n",
      "--------------------------------------------------\n",
      "TOTAL:                109,487,233 params\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "path = \"models/setfit-mpnet-irony\"\n",
    "model = SetFitModel.from_pretrained(path)\n",
    "\n",
    "def count_torch_params(module) -> int:\n",
    "    \"\"\"Count PyTorch parameters.\"\"\"\n",
    "    if module is None or not hasattr(module, \"parameters\"):\n",
    "        return 0\n",
    "    return sum(p.numel() for p in module.parameters())\n",
    "\n",
    "def count_sklearn_params(head) -> int:\n",
    "    \"\"\"Count sklearn LogisticRegression parameters.\"\"\"\n",
    "    if head is None:\n",
    "        return 0\n",
    "    n = 0\n",
    "    if hasattr(head, \"coef_\"):\n",
    "        n += head.coef_.size\n",
    "    if hasattr(head, \"intercept_\"):\n",
    "        n += head.intercept_.size\n",
    "    return n\n",
    "\n",
    "# --- Get components ---\n",
    "body = getattr(model, \"model_body\", None)\n",
    "head = getattr(model, \"model_head\", None)\n",
    "\n",
    "# --- Count ---\n",
    "n_body = count_torch_params(body)\n",
    "n_head_torch = count_torch_params(head)\n",
    "n_head_sklearn = count_sklearn_params(head)\n",
    "\n",
    "# --- Display ---\n",
    "print(\"=\" * 50)\n",
    "print(\"ðŸ“Š SetFit Model Parameters\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Body type: {type(body).__name__}\")\n",
    "print(f\"Head type: {type(head).__name__}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Body (MPNet):     {n_body:>15,} params\")\n",
    "print(f\"Head (sklearn):   {n_head_sklearn:>15,} params\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"TOTAL:            {n_body + n_head_sklearn:>15,} params\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Cleanup\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee86e8d2",
   "metadata": {},
   "source": [
    "## Analysis: SetFit (MPNet) vs SOTA\n",
    "\n",
    "### Performance Comparison\n",
    "\n",
    "| Metric | SetFit (MPNet) | SOTA (RoBERTa-124M) | Gap |\n",
    "|--------|----------------|---------------------|-----|\n",
    "| **F1 (Irony)** | 72.00% | 75.07% | -3.07 pts |\n",
    "| **Accuracy** | 74.00% | 78.57% | -4.57 pts |\n",
    "| **Recall (Irony)** | 83.00% | 81.35% | **+1.65 pts**|\n",
    "| **Parameters** | 109.5M | 125M | -12% |\n",
    "| **Inference Time** | 0.96s | 1.72s | **1.8x faster**|\n",
    "\n",
    "---\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "| Aspect | SetFit | SOTA | Verdict |\n",
    "|--------|--------|------|---------|\n",
    "| **Training Time** | ~34 min | Pre-trained on 124M tweets | SetFit = from scratch |\n",
    "| **Hyperparameter Tuning** | None (default values) | Optimized | Room for improvement |\n",
    "| **Fine-tuning required** | Simple contrastive learning | Full fine-tuning | SetFit simpler |\n",
    "| **GPU Memory** | ~4 GB | ~4 GB | Equivalent |\n",
    "\n",
    "---\n",
    "\n",
    "### SetFit Advantages\n",
    "\n",
    "1. **Fast training** â€” 34 min on RTX 4060 (vs hours for classic fine-tuning)\n",
    "2. **Few-shot capable** â€” Can work with very few examples\n",
    "3. **No hyperparameter tuning** â€” Competitive results with default config\n",
    "4. **Fast inference** â€” 1.8x faster than SOTA\n",
    "5. **High recall** â€” Better at detecting irony cases (+1.65 pts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a2500b",
   "metadata": {},
   "source": [
    "# Classic ML\n",
    "## Jina Embeddings v3 (512 dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba869f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”´ VRAM avant cleanup:\n",
      "   Allocated: 1.65 GB\n",
      "   Reserved:  7.40 GB\n",
      "   Deleted: model_setfit\n",
      "   Deleted: trainer\n",
      "   Deleted: raw_dataset\n",
      "   Deleted: train_raw\n",
      "   Deleted: val_raw\n",
      "   Deleted: test_raw\n",
      "\n",
      "âœ… VRAM aprÃ¨s cleanup:\n",
      "   Allocated: 0.43 GB\n",
      "   Reserved:  0.80 GB\n",
      "   Free:      7.19 GB\n",
      "   Total:     8.00 GB\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ðŸ§¹ AGGRESSIVE MEMORY CLEANUP\n",
    "# =============================================================================\n",
    "print(\"ðŸ”´ VRAM avant cleanup:\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    print(f\"   Reserved:  {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "\n",
    "# 1. Delete any remaining model variables\n",
    "vars_to_delete = [\n",
    "    'model_cardiff', 'tokenizer_cardiff', 'pipe_cardiff_base',\n",
    "    'model_setfit', 'trainer', 'encoder',\n",
    "    'best_rf', 'best_xgb', 'rf_search', 'xgb_search',\n",
    "    'X_train', 'X_val', 'X_test', 'X_full_train',\n",
    "    'raw_dataset', 'train_raw', 'val_raw', 'test_raw'\n",
    "]\n",
    "\n",
    "for var in vars_to_delete:\n",
    "    if var in globals():\n",
    "        del globals()[var]\n",
    "        print(f\"   Deleted: {var}\")\n",
    "\n",
    "# 2. Aggressive garbage collection\n",
    "gc.collect()\n",
    "gc.collect()\n",
    "gc.collect()\n",
    "\n",
    "# 3. Clear CUDA cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "# 4. Final check\n",
    "print(\"\\nâœ… VRAM aprÃ¨s cleanup:\")\n",
    "if torch.cuda.is_available():\n",
    "    allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "    reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "    total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"   Allocated: {allocated:.2f} GB\")\n",
    "    print(f\"   Reserved:  {reserved:.2f} GB\")\n",
    "    print(f\"   Free:      {total - reserved:.2f} GB\")\n",
    "    print(f\"   Total:     {total:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7509d21f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EXPERIMENT 3: Jina v3 Embeddings + Classic ML\n",
      "============================================================\n",
      "Encoder: jinaai/jina-embeddings-v3\n",
      "Task: classification | Dim: 768\n",
      "Device: cuda\n",
      "\n",
      "Loading Jina v3 encoder...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading raw dataset for embeddings...\n",
      "Encoding train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [00:13<00:00,  6.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding val...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:03<00:00,  8.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:03<00:00,  6.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: Train (2862, 768) | Val (955, 768) | Test (784, 768)\n",
      "Memory flushed. VRAM ready for the next round.\n",
      "Full training set: (3817, 768)\n",
      "\n",
      "============================================================\n",
      "Training XGBoost...\n",
      "============================================================\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anass\\Desktop\\irony-detection-nlp-benchmark\\.venv\\Lib\\site-packages\\xgboost\\core.py:774: UserWarning: [22:51:46] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\common\\error_msg.cc:62: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  return func(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost training complete in 1234.1s\n",
      "\n",
      "ðŸ“Š RESULTS: XGBoost (Jina v3 768d)\n",
      "  Accuracy: 66.84%\n",
      "  F1-Score (Irony): 61.08%\n",
      "  Inference Time: 0.04s\n",
      "----------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Not_Irony       0.75      0.68      0.71       473\n",
      "       Irony       0.57      0.66      0.61       311\n",
      "\n",
      "    accuracy                           0.67       784\n",
      "   macro avg       0.66      0.67      0.66       784\n",
      "weighted avg       0.68      0.67      0.67       784\n",
      "\n",
      "========================================\n",
      "XGBoost saved to models/xgb_jina_v3_768.joblib\n",
      "\n",
      "============================================================\n",
      "Training Random Forest...\n",
      "============================================================\n",
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n",
      "RF training complete in 818.1s\n",
      "\n",
      "ðŸ“Š RESULTS: Random Forest (Jina v3 768d)\n",
      "  Accuracy: 65.82%\n",
      "  F1-Score (Irony): 59.15%\n",
      "  Inference Time: 0.02s\n",
      "----------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Not_Irony       0.73      0.68      0.71       473\n",
      "       Irony       0.56      0.62      0.59       311\n",
      "\n",
      "    accuracy                           0.66       784\n",
      "   macro avg       0.65      0.65      0.65       784\n",
      "weighted avg       0.67      0.66      0.66       784\n",
      "\n",
      "========================================\n",
      "RF saved to models/rf_jina_v3_768.joblib\n",
      "\n",
      "============================================================\n",
      "BEST HYPERPARAMETERS\n",
      "============================================================\n",
      "\n",
      "--- XGBoost ---\n",
      "  subsample: 0.9\n",
      "  reg_lambda: 0.5\n",
      "  reg_alpha: 0.01\n",
      "  n_estimators: 300\n",
      "  max_depth: 3\n",
      "  learning_rate: 0.1\n",
      "  gamma: 0.2\n",
      "  colsample_bytree: 0.7\n",
      "\n",
      "--- Random Forest ---\n",
      "  n_estimators: 100\n",
      "  min_samples_split: 2\n",
      "  min_samples_leaf: 2\n",
      "  max_features: None\n",
      "  max_depth: 50\n",
      "  class_weight: balanced\n",
      "\n",
      "============================================================\n",
      "Memory flushed. VRAM ready for the next round.\n",
      "\n",
      "Experiment 3 complete!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# EXPERIMENT 3: Classic ML + Jina Embeddings v3\n",
    "# =============================================================================\n",
    "# Approach: Extract embeddings with Jina v3, then train XGBoost/RF classifiers.\n",
    "# This tests if high-quality embeddings + classic ML can compete with transformers.\n",
    "# =============================================================================\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer\n",
    "import joblib\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EXPERIMENT 3: Jina v3 Embeddings + Classic ML\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# --- Config ---\n",
    "MODEL_ID_JINA = \"jinaai/jina-embeddings-v3\"\n",
    "DIM = 768\n",
    "TASK = \"classification\"\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "print(f\"Encoder: {MODEL_ID_JINA}\")\n",
    "print(f\"Task: {TASK} | Dim: {DIM}\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "\n",
    "# --- Load Encoder ---\n",
    "print(\"\\nLoading Jina v3 encoder...\")\n",
    "encoder = SentenceTransformer(\n",
    "    MODEL_ID_JINA,\n",
    "    trust_remote_code=True,\n",
    "    device=DEVICE,\n",
    "    truncate_dim=DIM,\n",
    ")\n",
    "\n",
    "# --- Encode Function ---\n",
    "def encode_texts(enc, texts, task, batch_size, dim):\n",
    "    \"\"\"Encode texts with Jina v3.\"\"\"\n",
    "    return enc.encode(\n",
    "        texts,\n",
    "        convert_to_numpy=True,\n",
    "        show_progress_bar=True,\n",
    "        batch_size=batch_size,\n",
    "        normalize_embeddings=True,\n",
    "        task=task,\n",
    "        truncate_dim=dim,\n",
    "    )\n",
    "\n",
    "# --- Encode All Splits (use RAW data, not preprocessed) ---\n",
    "print(\"\\nLoading raw dataset for embeddings...\")\n",
    "raw_dataset = load_dataset(\"tweet_eval\", \"irony\")\n",
    "\n",
    "print(\"Encoding train...\")\n",
    "X_train = encode_texts(encoder, raw_dataset[\"train\"][\"text\"], TASK, BATCH_SIZE, DIM)\n",
    "y_train = np.array(raw_dataset[\"train\"][\"label\"], dtype=np.int64)\n",
    "\n",
    "print(\"Encoding val...\")\n",
    "X_val = encode_texts(encoder, raw_dataset[\"validation\"][\"text\"], TASK, BATCH_SIZE, DIM)\n",
    "y_val = np.array(raw_dataset[\"validation\"][\"label\"], dtype=np.int64)\n",
    "\n",
    "print(\"Encoding test...\")\n",
    "X_test = encode_texts(encoder, raw_dataset[\"test\"][\"text\"], TASK, BATCH_SIZE, DIM)\n",
    "y_test = np.array(raw_dataset[\"test\"][\"label\"], dtype=np.int64)\n",
    "\n",
    "print(f\"Embeddings shape: Train {X_train.shape} | Val {X_val.shape} | Test {X_test.shape}\")\n",
    "\n",
    "# --- Free VRAM ---\n",
    "del encoder\n",
    "flush_memory()\n",
    "\n",
    "# --- Merge train + val ---\n",
    "X_full_train = np.vstack([X_train, X_val])\n",
    "y_full_train = np.concatenate([y_train, y_val])\n",
    "print(f\"Full training set: {X_full_train.shape}\")\n",
    "\n",
    "# --- CV Setup ---\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=SEED)\n",
    "scorer = make_scorer(f1_score, pos_label=1, average=\"binary\")\n",
    "\n",
    "# =============================================================================\n",
    "# XGBOOST\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Training XGBoost...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "xgb_params = {\n",
    "    \"n_estimators\": [50, 100, 200, 300],\n",
    "    \"learning_rate\": [0.005, 0.01, 0.03, 0.05, 0.1],\n",
    "    \"max_depth\": [3, 5, 6, 8, 10],\n",
    "    \"subsample\": [0.7, 0.8, 0.9, 1.0],\n",
    "    \"colsample_bytree\": [0.7, 0.8, 0.9, 1.0],\n",
    "    \"gamma\": [0.0, 0.1, 0.2, 0.5, 1.0],\n",
    "    \"reg_lambda\": [0.0, 0.5, 1.0, 2.0],\n",
    "    \"reg_alpha\": [0.0, 0.01, 0.05, 0.1, 0.5],\n",
    "}\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    random_state=SEED,\n",
    "    eval_metric=\"logloss\",\n",
    "    tree_method=\"hist\",\n",
    "    device=DEVICE,\n",
    "    n_jobs=1,\n",
    ")\n",
    "\n",
    "start_xgb = time.time()\n",
    "xgb_search = RandomizedSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_distributions=xgb_params,\n",
    "    n_iter=100,\n",
    "    cv=cv,\n",
    "    scoring=scorer,\n",
    "    verbose=1,\n",
    "    random_state=SEED,\n",
    "    n_jobs=1,\n",
    "    refit=True,\n",
    ")\n",
    "xgb_search.fit(X_full_train, y_full_train)\n",
    "xgb_train_time = time.time() - start_xgb\n",
    "\n",
    "best_xgb = xgb_search.best_estimator_\n",
    "best_xgb_params = xgb_search.best_params_\n",
    "print(f\"XGBoost training complete in {xgb_train_time:.1f}s\")\n",
    "\n",
    "# --- XGBoost Inference ---\n",
    "start_inf = time.time()\n",
    "y_pred_xgb = best_xgb.predict(X_test)\n",
    "xgb_inference_time = time.time() - start_inf\n",
    "\n",
    "# --- XGBoost Evaluation ---\n",
    "evaluate_model(y_test, y_pred_xgb, f\"XGBoost (Jina v3 {DIM}d)\", xgb_inference_time)\n",
    "\n",
    "# --- Save XGBoost ---\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "joblib.dump(best_xgb, f\"models/xgb_jina_v3_{DIM}.joblib\")\n",
    "print(f\"XGBoost saved to models/xgb_jina_v3_{DIM}.joblib\")\n",
    "\n",
    "# =============================================================================\n",
    "# RANDOM FOREST\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Training Random Forest...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "rf_params = {\n",
    "    \"n_estimators\": [50, 100, 200, 300],\n",
    "    \"max_depth\": [None, 10, 20, 30, 50],\n",
    "    \"min_samples_split\": [2, 5, 10, 20],\n",
    "    \"min_samples_leaf\": [1, 2, 4, 8],\n",
    "    \"max_features\": [\"sqrt\", \"log2\", None],\n",
    "    \"class_weight\": [\"balanced\"],\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(random_state=SEED, n_jobs=1)\n",
    "\n",
    "start_rf = time.time()\n",
    "rf_search = RandomizedSearchCV(\n",
    "    estimator=rf,\n",
    "    param_distributions=rf_params,\n",
    "    n_iter=50,\n",
    "    cv=cv,\n",
    "    scoring=scorer,\n",
    "    verbose=1,\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    ")\n",
    "rf_search.fit(X_full_train, y_full_train)\n",
    "rf_train_time = time.time() - start_rf\n",
    "\n",
    "best_rf = rf_search.best_estimator_\n",
    "best_rf_params = rf_search.best_params_\n",
    "print(f\"RF training complete in {rf_train_time:.1f}s\")\n",
    "\n",
    "# --- RF Inference ---\n",
    "start_inf = time.time()\n",
    "y_pred_rf = best_rf.predict(X_test)\n",
    "rf_inference_time = time.time() - start_inf\n",
    "\n",
    "# --- RF Evaluation ---\n",
    "evaluate_model(y_test, y_pred_rf, f\"Random Forest (Jina v3 {DIM}d)\", rf_inference_time)\n",
    "\n",
    "# --- Save RF ---\n",
    "joblib.dump(best_rf, f\"models/rf_jina_v3_{DIM}.joblib\")\n",
    "print(f\"RF saved to models/rf_jina_v3_{DIM}.joblib\")\n",
    "\n",
    "# =============================================================================\n",
    "# BEST HYPERPARAMETERS SUMMARY\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"BEST HYPERPARAMETERS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n--- XGBoost ---\")\n",
    "for param, value in best_xgb_params.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "print(\"\\n--- Random Forest ---\")\n",
    "for param, value in best_rf_params.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "# --- Cleanup ---\n",
    "del X_train, X_val, X_test, X_full_train\n",
    "del best_rf, best_xgb, rf_search, xgb_search\n",
    "flush_memory()\n",
    "\n",
    "print(\"\\nExperiment 3 complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d434ebd",
   "metadata": {},
   "source": [
    "## Jina Embeddings v3 (256 dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8930f367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EXPERIMENT 3b: Jina v3 Embeddings + Classic ML (256d)\n",
      "============================================================\n",
      "Encoder: jinaai/jina-embeddings-v3\n",
      "Task: classification | Dim: 256\n",
      "Device: cuda\n",
      "\n",
      "Loading Jina v3 encoder...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n",
      "flash_attn is not installed. Using PyTorch native attention implementation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading raw dataset for embeddings...\n",
      "Encoding train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90/90 [00:10<00:00,  8.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding val...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:03<00:00,  8.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:02<00:00,  8.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: Train (2862, 256) | Val (955, 256) | Test (784, 256)\n",
      "Memory flushed. VRAM ready for the next round.\n",
      "Full training set: (3817, 256)\n",
      "\n",
      "============================================================\n",
      "Training XGBoost...\n",
      "============================================================\n",
      "Fitting 3 folds for each of 200 candidates, totalling 600 fits\n",
      "XGBoost training complete in 1694.9s\n",
      "\n",
      "ðŸ“Š RESULTS: XGBoost (Jina v3 256d)\n",
      "  Accuracy: 71.43%\n",
      "  F1-Score (Irony): 67.44%\n",
      "  Inference Time: 0.03s\n",
      "----------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Not_Irony       0.81      0.69      0.75       473\n",
      "       Irony       0.62      0.75      0.67       311\n",
      "\n",
      "    accuracy                           0.71       784\n",
      "   macro avg       0.71      0.72      0.71       784\n",
      "weighted avg       0.73      0.71      0.72       784\n",
      "\n",
      "========================================\n",
      "XGBoost saved to models/xgb_jina_v3_256.joblib\n",
      "\n",
      "============================================================\n",
      "Training Random Forest...\n",
      "============================================================\n",
      "Fitting 3 folds for each of 70 candidates, totalling 210 fits\n",
      "RF training complete in 974.5s\n",
      "\n",
      "ðŸ“Š RESULTS: Random Forest (Jina v3 256d)\n",
      "  Accuracy: 65.69%\n",
      "  F1-Score (Irony): 58.16%\n",
      "  Inference Time: 0.06s\n",
      "----------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Not_Irony       0.73      0.69      0.71       473\n",
      "       Irony       0.56      0.60      0.58       311\n",
      "\n",
      "    accuracy                           0.66       784\n",
      "   macro avg       0.64      0.65      0.65       784\n",
      "weighted avg       0.66      0.66      0.66       784\n",
      "\n",
      "========================================\n",
      "RF saved to models/rf_jina_v3_256.joblib\n",
      "\n",
      "============================================================\n",
      "BEST HYPERPARAMETERS (256 dimensions)\n",
      "============================================================\n",
      "\n",
      "--- XGBoost ---\n",
      "  subsample: 0.8\n",
      "  reg_lambda: 2.0\n",
      "  reg_alpha: 0.01\n",
      "  n_estimators: 450\n",
      "  max_depth: 6\n",
      "  max_bin: 256\n",
      "  learning_rate: 0.03\n",
      "  gamma: 0.0\n",
      "  colsample_bytree: 0.8\n",
      "\n",
      "--- Random Forest ---\n",
      "  n_estimators: 300\n",
      "  min_samples_split: 5\n",
      "  min_samples_leaf: 2\n",
      "  max_features: sqrt\n",
      "  max_depth: None\n",
      "  class_weight: balanced\n",
      "\n",
      "============================================================\n",
      "Memory flushed. VRAM ready for the next round.\n",
      "\n",
      "Experiment 3b complete!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# EXPERIMENT 3b: Classic ML + Jina Embeddings v3 (256 dimensions)\n",
    "# =============================================================================\n",
    "# Same approach as Experiment 3, but with reduced dimensionality.\n",
    "# Tests if Matryoshka truncation to 256d affects performance significantly.\n",
    "# =============================================================================\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import make_scorer\n",
    "import joblib\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EXPERIMENT 3b: Jina v3 Embeddings + Classic ML (256d)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# --- Config ---\n",
    "MODEL_ID_JINA = \"jinaai/jina-embeddings-v3\"\n",
    "DIM = 256\n",
    "TASK = \"classification\"\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "print(f\"Encoder: {MODEL_ID_JINA}\")\n",
    "print(f\"Task: {TASK} | Dim: {DIM}\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "\n",
    "# --- Load Encoder ---\n",
    "print(\"\\nLoading Jina v3 encoder...\")\n",
    "encoder = SentenceTransformer(\n",
    "    MODEL_ID_JINA,\n",
    "    trust_remote_code=True,\n",
    "    device=DEVICE,\n",
    "    truncate_dim=DIM,\n",
    ")\n",
    "\n",
    "# --- Encode Function ---\n",
    "def encode_texts(enc, texts, task, batch_size, dim):\n",
    "    \"\"\"Encode texts with Jina v3.\"\"\"\n",
    "    return enc.encode(\n",
    "        texts,\n",
    "        convert_to_numpy=True,\n",
    "        show_progress_bar=True,\n",
    "        batch_size=batch_size,\n",
    "        normalize_embeddings=True,\n",
    "        task=task,\n",
    "        truncate_dim=dim,\n",
    "    )\n",
    "\n",
    "# --- Encode All Splits (use RAW data, not preprocessed) ---\n",
    "print(\"\\nLoading raw dataset for embeddings...\")\n",
    "raw_dataset = load_dataset(\"tweet_eval\", \"irony\")\n",
    "\n",
    "print(\"Encoding train...\")\n",
    "X_train = encode_texts(encoder, raw_dataset[\"train\"][\"text\"], TASK, BATCH_SIZE, DIM)\n",
    "y_train = np.array(raw_dataset[\"train\"][\"label\"], dtype=np.int64)\n",
    "\n",
    "print(\"Encoding val...\")\n",
    "X_val = encode_texts(encoder, raw_dataset[\"validation\"][\"text\"], TASK, BATCH_SIZE, DIM)\n",
    "y_val = np.array(raw_dataset[\"validation\"][\"label\"], dtype=np.int64)\n",
    "\n",
    "print(\"Encoding test...\")\n",
    "X_test = encode_texts(encoder, raw_dataset[\"test\"][\"text\"], TASK, BATCH_SIZE, DIM)\n",
    "y_test = np.array(raw_dataset[\"test\"][\"label\"], dtype=np.int64)\n",
    "\n",
    "print(f\"Embeddings shape: Train {X_train.shape} | Val {X_val.shape} | Test {X_test.shape}\")\n",
    "\n",
    "# --- Free VRAM ---\n",
    "del encoder\n",
    "flush_memory()\n",
    "\n",
    "# --- Merge train + val ---\n",
    "X_full_train = np.vstack([X_train, X_val])\n",
    "y_full_train = np.concatenate([y_train, y_val])\n",
    "print(f\"Full training set: {X_full_train.shape}\")\n",
    "\n",
    "# --- CV Setup ---\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=SEED)\n",
    "scorer = make_scorer(f1_score, pos_label=1, average=\"binary\")\n",
    "\n",
    "# =============================================================================\n",
    "# XGBOOST\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Training XGBoost...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "xgb_params = {\n",
    "    \"n_estimators\": [100, 200, 250, 300, 350, 400, 450, 500],\n",
    "    \"learning_rate\": [0.005, 0.01, 0.03, 0.05, 0.1],\n",
    "    \"max_depth\": [3, 5, 6, 8, 10],\n",
    "    \"subsample\": [0.7, 0.8, 0.9, 1.0],\n",
    "    \"colsample_bytree\": [0.7, 0.8, 0.9, 1.0],\n",
    "    \"gamma\": [0.0, 0.1, 0.2, 0.5, 1.0],\n",
    "    \"reg_lambda\": [0.0, 0.5, 1.0, 2.0],\n",
    "    \"reg_alpha\": [0.0, 0.01, 0.05, 0.1, 0.5],\n",
    "    \"max_bin\": [128, 256],\n",
    "}\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    random_state=SEED,\n",
    "    eval_metric=\"logloss\",\n",
    "    tree_method=\"hist\",\n",
    "    device=DEVICE,\n",
    "    n_jobs=1,\n",
    ")\n",
    "\n",
    "start_xgb = time.time()\n",
    "xgb_search = RandomizedSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_distributions=xgb_params,\n",
    "    n_iter=200,\n",
    "    cv=cv,\n",
    "    scoring=scorer,\n",
    "    verbose=1,\n",
    "    random_state=SEED,\n",
    "    n_jobs=1,\n",
    "    refit=True,\n",
    ")\n",
    "xgb_search.fit(X_full_train, y_full_train)\n",
    "xgb_train_time = time.time() - start_xgb\n",
    "\n",
    "best_xgb = xgb_search.best_estimator_\n",
    "best_xgb_params = xgb_search.best_params_\n",
    "print(f\"XGBoost training complete in {xgb_train_time:.1f}s\")\n",
    "\n",
    "# --- XGBoost Inference ---\n",
    "start_inf = time.time()\n",
    "y_pred_xgb = best_xgb.predict(X_test)\n",
    "xgb_inference_time = time.time() - start_inf\n",
    "\n",
    "# --- XGBoost Evaluation ---\n",
    "evaluate_model(y_test, y_pred_xgb, f\"XGBoost (Jina v3 {DIM}d)\", xgb_inference_time)\n",
    "\n",
    "# --- Save XGBoost ---\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "joblib.dump(best_xgb, f\"models/xgb_jina_v3_{DIM}.joblib\")\n",
    "print(f\"XGBoost saved to models/xgb_jina_v3_{DIM}.joblib\")\n",
    "\n",
    "# =============================================================================\n",
    "# RANDOM FOREST\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Training Random Forest...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "rf_params = {\n",
    "    \"n_estimators\": [100, 200, 250, 300],\n",
    "    \"max_depth\": [None, 10, 30, 50, 70],\n",
    "    \"min_samples_split\": [2, 5, 10, 20],\n",
    "    \"min_samples_leaf\": [1, 2, 4, 8],\n",
    "    \"max_features\": [\"sqrt\", \"log2\", None],\n",
    "    \"class_weight\": [\"balanced\"],\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier(random_state=SEED, n_jobs=1)\n",
    "\n",
    "start_rf = time.time()\n",
    "rf_search = RandomizedSearchCV(\n",
    "    estimator=rf,\n",
    "    param_distributions=rf_params,\n",
    "    n_iter=70,\n",
    "    cv=cv,\n",
    "    scoring=scorer,\n",
    "    verbose=1,\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    ")\n",
    "rf_search.fit(X_full_train, y_full_train)\n",
    "rf_train_time = time.time() - start_rf\n",
    "\n",
    "best_rf = rf_search.best_estimator_\n",
    "best_rf_params = rf_search.best_params_\n",
    "print(f\"RF training complete in {rf_train_time:.1f}s\")\n",
    "\n",
    "# --- RF Inference ---\n",
    "start_inf = time.time()\n",
    "y_pred_rf = best_rf.predict(X_test)\n",
    "rf_inference_time = time.time() - start_inf\n",
    "\n",
    "# --- RF Evaluation ---\n",
    "evaluate_model(y_test, y_pred_rf, f\"Random Forest (Jina v3 {DIM}d)\", rf_inference_time)\n",
    "\n",
    "# --- Save RF ---\n",
    "joblib.dump(best_rf, f\"models/rf_jina_v3_{DIM}.joblib\")\n",
    "print(f\"RF saved to models/rf_jina_v3_{DIM}.joblib\")\n",
    "\n",
    "# =============================================================================\n",
    "# BEST HYPERPARAMETERS SUMMARY\n",
    "# =============================================================================\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"BEST HYPERPARAMETERS (256 dimensions)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n--- XGBoost ---\")\n",
    "for param, value in best_xgb_params.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "print(\"\\n--- Random Forest ---\")\n",
    "for param, value in best_rf_params.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "# --- Cleanup ---\n",
    "del X_train, X_val, X_test, X_full_train\n",
    "del best_rf, best_xgb, rf_search, xgb_search\n",
    "flush_memory()\n",
    "\n",
    "print(\"\\nExperiment 3b complete!\")              "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17112b9c",
   "metadata": {},
   "source": [
    "## Analysis: Classic ML vs Fine-tuned Transformers\n",
    "\n",
    "### Results Summary\n",
    "\n",
    "| Model | Accuracy | F1 (Irony) | Inference Time | Parameters |\n",
    "|-------|----------|------------|----------------|------------|\n",
    "| **Cardiff RoBERTa-2021-124M (SOTA)** | **78.57%** | **75.07%** | 1.72s | 125M |\n",
    "| SetFit (MPNet) | 74.00% | 72.00% | 0.96s | 109M |\n",
    "| Cardiff RoBERTa-Base | 73.47% | 62.72% | 9.54s | 125M |\n",
    "| **XGBoost (Jina v3 256d)** | **71.43%** | **67.44%** | 0.03s | <1M |\n",
    "| XGBoost (Jina v3 768d) | 66.84% | 61.08% | 0.04s | <1M |\n",
    "| Random Forest (Jina v3 768d) | 65.82% | 59.15% | 0.02s | <1M |\n",
    "| Random Forest (Jina v3 256d) | 65.69% | 58.16% | 0.06s | <1M |\n",
    "\n",
    "---\n",
    "\n",
    "### Key Finding: XGBoost 256d vs Cardiff Base\n",
    "\n",
    "| Metric | XGBoost 256d | Cardiff Base | Difference |\n",
    "|--------|--------------|--------------|------------|\n",
    "| F1 (Irony) | 67.44% | 62.72% | **+4.7 pts** |\n",
    "| Accuracy | 71.43% | 73.47% | -2.0 pts |\n",
    "| Inference | 0.03s | 9.54s | **318x faster** |\n",
    "| Parameters | <1M | 125M | **125x smaller** |\n",
    "\n",
    "XGBoost 256d **outperforms** Cardiff RoBERTa-Base on F1 score while being 318x faster and 125x smaller.\n",
    "\n",
    "---\n",
    "\n",
    "### Why Classic ML Underperforms vs SOTA\n",
    "\n",
    "Classic ML models (RF, XGBoost) cannot capture the **contextual and semantic nuances** required for irony detection. Irony relies on subtle linguistic patterns, tone shifts, and implicit meaning that transformers learn through pre-training on massive corpora. Tree-based models only see fixed embeddings without context adaptation.\n",
    "\n",
    "---\n",
    "\n",
    "### Why 256d Outperforms 768d (Curse of Dimensionality)\n",
    "\n",
    "| Dimension | Features | Training Samples | Samples/Feature Ratio |\n",
    "|-----------|----------|------------------|----------------------|\n",
    "| 768d | 768 | 3,817 | ~5:1 |\n",
    "| 256d | 256 | 3,817 | ~15:1 |\n",
    "\n",
    "With 768 features and only ~3,800 samples, tree-based models suffer from:\n",
    "- **Overfitting**: Too many features relative to observations\n",
    "- **Sparse splits**: Trees cannot find robust decision boundaries\n",
    "- **Noise amplification**: Higher dimensions contain more noise\n",
    "\n",
    "256 dimensions provide a better signal-to-noise ratio for the limited dataset size.\n",
    "\n",
    "---\n",
    "\n",
    "### Jina v3 Matryoshka Embeddings: Relative Performance\n",
    "\n",
    "Jina v3 uses Matryoshka Representation Learning, allowing truncation to smaller dimensions with minimal quality loss:\n",
    "\n",
    "| Dimension | Relative Performance | Use Case |\n",
    "|-----------|---------------------|----------|\n",
    "| 1024 (Full) | 100% | Maximum precision tasks |\n",
    "| 768 | ~99.5% | General purpose |\n",
    "| 512 | ~99.0% | Balanced |\n",
    "| 256 | ~97.5% | Classification (sufficient) |\n",
    "| 128 | ~95.0% | Lightweight/mobile |\n",
    "\n",
    "For **binary classification** tasks like irony detection, 256 dimensions retain sufficient discriminative power while significantly reducing computational overhead.\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "| Aspect | XGBoost 256d | Verdict |\n",
    "|--------|--------------|---------|\n",
    "| **Performance** | 67.44% F1 | Comparable to Cardiff Base, ~10% below SOTA |\n",
    "| **Speed** | 0.03s inference | 57x faster than SOTA |\n",
    "| **Size** | <1MB model | 125x smaller than transformers |\n",
    "| **Complexity** | Simple tree ensemble | No GPU required |\n",
    "| **Training** | ~28 min | No pre-training needed |\n",
    "\n",
    "**XGBoost 256d is a strong baseline** for resource-constrained environments where transformer inference is not feasible. It demonstrates that high-quality embeddings + simple classifiers can compete with older fine-tuned models, but cannot match modern SOTA transformers on nuanced NLP tasks like irony detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaeed567",
   "metadata": {},
   "source": [
    "# Large Language Model (`Qwen 2.5-7b-instruct`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b761d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³ Chargement de TOUT le dataset TweetEval (Irony)...\n",
      "ðŸ“¦ Fusion des donnÃ©es...\n",
      "âœ… PrÃªt Ã  traiter 3646 tweets cette nuit.\n",
      "\n",
      "ðŸš€ C'est parti ! Bonne nuit. (Ctrl+C pour arrÃªter si besoin)\n",
      "\n",
      "âœ…âŒâŒâŒâœ…âœ…âœ…âœ…âœ…âœ…âŒâŒâœ…âœ…âœ…âŒâœ…âŒâœ…âŒâŒâŒâŒâœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âŒâœ…âŒâŒâœ…âœ…âŒâœ…âœ…âŒâŒâŒâœ… [SauvegardÃ© 50/3646]âœ…âœ…âŒâŒâŒâŒâœ…âœ…âŒâœ…âŒâœ…âœ…âŒâœ…âŒâŒâŒâŒâŒâœ…âœ…âœ…âœ…âŒâœ…âœ…âœ…âœ…âœ…âœ…âŒâœ…âœ…âœ…âœ…âŒâŒâœ…âŒâŒâœ…âœ…âœ…âŒâœ…âœ…âŒâœ…âœ… [SauvegardÃ© 100/3646]âœ…âœ…âœ…âœ…âœ…âœ…âŒâŒâŒâŒâœ…âœ…âŒâŒâœ…âœ…âœ…âœ…âœ…âœ…âŒâœ…âŒâŒâœ…âœ…âŒâœ…âŒâœ…âœ…âœ…âœ…âŒâœ…âŒâŒâœ…âœ…âœ…âœ…âœ…âœ…âœ…âŒâŒâœ…âœ…âŒâœ… [SauvegardÃ© 150/3646]âŒâŒâŒâœ…âŒâœ…âœ…âœ…âœ…âœ…âœ…âŒâœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âŒâœ…âœ…âœ…âœ…âŒâœ…âœ…âœ…âœ…âœ…âŒâœ…âŒâŒâœ…âœ…âœ…âŒâŒ [SauvegardÃ© 200/3646]âœ…âŒâŒâŒâœ…âŒâœ…âœ…âœ…âœ…âœ…âŒâŒâœ…âŒâœ…âœ…âŒâœ…âŒâœ…âœ…âŒâœ…âŒâŒâŒâœ…âŒâœ…âœ…âŒâŒâœ…âŒâœ…âŒâœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âŒâŒâŒâŒâŒ [SauvegardÃ© 250/3646]âœ…âœ…âœ…âœ…âœ…âœ…âœ…âŒâœ…âœ…âŒâŒâœ…âœ…âŒâŒâŒâœ…âœ…âœ…âœ…âŒâœ…âŒâœ…âŒâœ…âŒâŒâœ…âœ…âŒâŒâŒâœ…âŒâœ…âœ…âœ…âŒâŒâœ…âŒâŒâŒâœ…âŒâœ…âœ…âŒ [SauvegardÃ© 300/3646]âŒâœ…âœ…âŒâŒâœ…âŒâŒâŒâœ…âŒâœ…âœ…âŒâœ…âŒâŒâŒâœ…âœ…âœ…âŒâŒâŒâœ…âŒâœ…âœ…âœ…âœ…âœ…âœ…âŒâŒâŒâœ…âœ…âŒâŒâœ…âœ…âŒâŒâŒâœ…âŒâœ…âŒâœ…âœ… [SauvegardÃ© 350/3646]âœ…âœ…âœ…âœ…âœ…âœ…âŒâœ…âœ…âœ…âœ…âœ…âŒâŒâœ…âŒâŒâœ…âŒâœ…âœ…âŒâŒâŒâœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âŒâœ…âœ…âŒâœ…âœ…âŒâŒâœ…âŒâœ…âœ…âŒâŒâœ…âŒâœ…âŒ [SauvegardÃ© 400/3646]âœ…âœ…âŒâœ…âŒâœ…âœ…âŒâŒâŒâœ…âœ…âŒâœ…âœ…âŒâœ…âœ…âœ…âŒâŒâœ…âŒâŒâœ…âœ…âŒâœ…âŒâŒâœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âŒâŒâŒâœ…âœ…âœ…âœ…âŒâœ…âœ…âœ…âœ… [SauvegardÃ© 450/3646]âœ…âœ…âŒâœ…âœ…âœ…âŒâŒâœ…âŒâœ…âŒâœ…âœ…âŒâœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âŒâœ…âœ…âœ…âœ…âœ…âŒâœ…âŒâŒâŒâœ…âœ…âŒâŒâœ…âŒâœ…âœ…âœ…âœ…âœ…âœ…âœ…âŒâŒâœ… [SauvegardÃ© 500/3646]âŒâœ…âœ…âœ…âŒâœ…âœ…âŒâœ…âŒâŒâœ…âœ…âœ…âœ…âŒâŒâŒâœ…âœ…âœ…âŒâœ…âœ…âŒâŒâœ…âœ…âœ…âŒâŒâŒâœ…âœ…âŒâŒâœ…âŒâœ…âœ…âœ…âœ…âŒâœ…âœ…âŒâŒâŒâŒâœ… [SauvegardÃ© 550/3646]âœ…âœ…âŒâœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âŒâŒâŒâœ…âœ…âŒâœ…âœ…âœ…âŒâŒâŒâŒâœ…âœ…âœ…âœ…âŒâœ…âœ…âœ…âœ…âŒâœ…âœ…âŒâœ…âŒâŒâœ…âœ…âœ…âœ…âŒâŒâŒ [SauvegardÃ© 600/3646]âœ…âœ…âœ…âœ…âŒâŒâŒâœ…âœ…âœ…âœ…âŒâœ…âŒâœ…âŒâœ…âœ…âœ…âŒâœ…âœ…âŒâœ…âœ…âœ…âŒâœ…âŒâœ…âŒâœ…âœ…âœ…âŒâœ…âŒâŒâœ…âœ…âœ…âœ…âœ…âŒâœ…âœ…âœ…âœ…âŒâŒ [SauvegardÃ© 650/3646]âŒâŒâœ…âœ…âœ…âŒâŒâŒâœ…âœ…âŒâŒâŒâŒâœ…âŒâœ…âœ…âœ…âŒâœ…âœ…âœ…âœ…âŒâŒâœ…âŒâŒâŒâœ…âœ…âœ…âœ…âœ…âœ…âŒâŒâœ…âŒâŒâœ…âœ…âœ…âœ…âœ…âœ…âŒâœ…âœ… [SauvegardÃ© 700/3646]âŒâœ…âœ…âœ…âŒâŒâŒâŒâŒâœ…âœ…âœ…âœ…âœ…âŒâœ…âŒâœ…âŒâŒâŒâœ…âœ…âœ…âŒâŒâœ…âœ…âœ…âœ…âŒâŒâœ…âœ…âœ…âœ…âŒâŒâŒâŒâœ…âŒâŒâœ…âŒâŒâŒâœ…âœ…âŒ [SauvegardÃ© 750/3646]âœ…âœ…âœ…âŒâŒâœ…âœ…âœ…âœ…âœ…âœ…âœ…âŒâŒâœ…âœ…âŒâŒâœ…âœ…âŒâŒâœ…âœ…âœ…âŒâœ…âœ…âœ…âœ…âŒâœ…âŒâŒâŒâŒâŒâœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âŒâŒâŒâœ… [SauvegardÃ© 800/3646]âŒâŒâœ…âŒâœ…âœ…âŒâœ…âœ…âŒâœ…âœ…âŒâœ…âœ…âœ…âœ…âœ…âœ…âŒâœ…âŒâœ…âŒâœ…âœ…âŒâŒâœ…âœ…âŒâœ…âŒâœ…âŒâœ…âœ…âœ…âœ…âŒâœ…âœ…âœ…âœ…âœ…âŒâŒâœ…âœ…âŒ [SauvegardÃ© 850/3646]âœ…âœ…âœ…âœ…âœ…âœ…âœ…âŒâœ…âœ…âŒâŒâŒâœ…âœ…âŒâœ…âœ…âŒâœ…âœ…âœ…âŒâŒâœ…âœ…âœ…âœ…âŒâŒâŒâœ…âŒâœ…âœ…âœ…âœ…âœ…âœ…âœ…âŒâœ…âœ…âŒâŒâŒâŒâœ…âœ…âŒ [SauvegardÃ© 900/3646]âœ…âœ…âœ…âœ…âœ…âŒâŒâœ…âœ…âœ…âŒâœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âŒâœ…âŒâœ…âœ…âœ…âŒâœ…âŒâœ…âŒâœ…âŒâœ…âœ…âŒâœ…âŒâŒâŒâŒâŒâœ…âœ…âœ…âŒâœ…âœ…âœ… [SauvegardÃ© 950/3646]âœ…âœ…âŒâŒâœ…âŒâœ…âŒâœ…âœ…âŒâœ…âœ…âœ…âŒâœ…âœ…âœ…âŒâŒâœ…âœ…âŒâœ…âœ…âŒâœ…âŒâœ…âœ…âŒâŒâœ…âœ…âœ…âœ…âœ…âŒâŒâœ…âœ…âœ…âœ…âœ…âŒâœ…âœ…âœ…âŒâœ… [SauvegardÃ© 1000/3646]âœ…âœ…âŒâœ…âœ…âœ…âœ…âŒâŒâœ…âŒâŒâŒâœ…âœ…âœ…âŒâœ…âŒâŒâŒâŒâœ…âŒâœ…âŒâœ…âœ…âœ…âŒâœ…âŒâŒâŒâœ…âœ…âœ…âœ…âŒâœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âŒ [SauvegardÃ© 1050/3646]âŒâœ…âœ…âœ…âŒâŒâœ…âœ…âœ…âŒâœ…âœ…âŒâŒâŒâœ…âœ…âœ…âŒâŒâœ…âŒâœ…âŒâœ…âœ…âŒâœ…âœ…âŒâŒâŒâœ…âœ…âœ…âœ…âŒâŒâŒâŒâœ…âŒâŒâŒâœ…âœ…âŒâŒâŒâŒ [SauvegardÃ© 1100/3646]âœ…âœ…âœ…âœ…âŒâœ…âœ…âŒâŒâŒâŒâœ…âŒâŒâŒâœ…âœ…âŒâŒâŒâœ…âœ…âœ…âŒâœ…âœ…âœ…âŒâœ…âœ…âœ…âŒâœ…âœ…âŒâŒâœ…âœ…âœ…âŒâŒâœ…âŒâœ…âœ…âŒâœ…âœ…âœ…âœ… [SauvegardÃ© 1150/3646]âŒâŒâœ…âœ…âŒâœ…âœ…âœ…âŒâŒâœ…âŒâŒâœ…âŒâœ…âŒâŒâœ…âœ…âŒâŒâœ…âŒâŒâŒâŒâœ…âŒâŒâŒâŒâŒâœ…âœ…âœ…âœ…âŒâŒâŒâœ…âœ…âœ…âŒâœ…âŒâœ…âŒâœ…âœ… [SauvegardÃ© 1200/3646]âŒâœ…âŒâœ…âŒâœ…âŒâœ…âœ…âœ…âŒâœ…âœ…âœ…âœ…âŒâœ…âœ…âŒâœ…âŒâŒâœ…âœ…âœ…âœ…âœ…âŒâœ…âŒâœ…âŒâœ…âŒâœ…âŒâŒâŒâœ…âŒâŒâŒâœ…âœ…âœ…âœ…âœ…âŒâŒâŒ [SauvegardÃ© 1250/3646]âŒâŒâŒâœ…âŒâœ…âœ…âŒâœ…âœ…âŒâœ…âœ…âŒâœ…âœ…âœ…âœ…âŒâŒâŒâœ…âœ…âœ…âœ…âŒâœ…âœ…âŒâŒâœ…âœ…âŒâœ…âŒâŒâŒâœ…âŒâœ…âŒâœ…âœ…âŒâŒâœ…âœ…âœ…âœ…âœ… [SauvegardÃ© 1300/3646]âœ…âŒâŒâœ…âœ…âœ…âœ…âŒâŒâŒâœ…âœ…âŒâŒâŒâŒâŒâŒâœ…âŒâŒâœ…âŒâŒâœ…âœ…âœ…âœ…âœ…âœ…âŒâŒâœ…âœ…âŒâœ…âœ…âœ…âœ…âœ…âŒâŒâŒâŒâœ…âŒâŒâŒâœ…âœ… [SauvegardÃ© 1350/3646]âœ…âŒâœ…âŒâŒâœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âŒâœ…âœ…âœ…âŒâœ…âŒâŒâŒâœ…âœ…âœ…âœ…âŒâœ…âŒâœ…âœ…âŒâœ…âœ…âœ…âŒâœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âŒâœ…âŒâŒâœ…âœ… [SauvegardÃ© 1400/3646]âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âŒâœ…âŒâœ…âœ…âœ…âŒâœ…âŒâŒâœ…âœ…âœ…âœ…âŒâŒâŒâŒâœ…âŒâœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âŒâœ…âŒâŒâœ…âœ…âœ…âŒâŒâŒ [SauvegardÃ© 1450/3646]âŒâœ…âœ…âŒâŒâŒâŒâœ…âœ…âœ…âœ…âœ…âŒâœ…âœ…âŒâŒâœ…âœ…âŒâŒâœ…âœ…âœ…âŒâŒâœ…âœ…âŒâŒâŒâŒâœ…âœ…âœ…âœ…âœ…âŒâœ…âœ…âœ…âœ…âŒâŒâŒâœ…âœ…âŒâŒâŒ [SauvegardÃ© 1500/3646]âœ…âœ…âœ…âœ…âœ…âŒâœ…âŒâœ…âŒâŒâœ…âœ…âœ…âœ…âŒâŒâœ…âŒâœ…âŒâŒâŒâœ…âœ…âœ…âŒâœ…âŒâŒâœ…âŒâœ…âœ…âŒâœ…âœ…âœ…âŒâŒâœ…âœ…âœ…âœ…âŒâœ…âœ…âŒâŒâœ… [SauvegardÃ© 1550/3646]âŒâŒâœ…âœ…âœ…âœ…âŒâœ…âœ…âŒâœ…âŒâœ…âœ…âœ…âœ…âœ…âœ…âœ…âŒâœ…âœ…âœ…âŒâœ…âœ…âŒâœ…âŒâœ…âœ…âŒâœ…âœ…âœ…âŒâŒâœ…âŒâœ…âœ…âœ…âœ…âœ…âŒâœ…âœ…âœ…âœ…âœ… [SauvegardÃ© 1600/3646]âœ…âœ…âœ…âœ…âŒâŒâŒâŒâœ…âœ…âœ…âœ…âœ…âŒâœ…âœ…âœ…âŒâŒâŒâŒâŒâœ…âœ…âœ…âœ…âœ…âŒâŒâœ…âŒâœ…âœ…âŒâŒâœ…âœ…âœ…âœ…âŒâœ…âœ…âŒâŒâŒâœ…âŒâœ…âœ…âœ… [SauvegardÃ© 1650/3646]âŒâŒâŒâœ…âœ…âŒâœ…âœ…âœ…âœ…âœ…âœ…âŒâŒâœ…âŒâŒâœ…âœ…âŒâœ…âœ…âœ…âœ…âœ…âŒâœ…âœ…âœ…âœ…âŒâŒâœ…âœ…âŒâŒâœ…âŒâœ…âœ…âœ…âœ…âœ…âœ…âŒâœ…âœ…âœ…âœ…âŒ [SauvegardÃ© 1700/3646]âŒâŒâŒâœ…âŒâŒâœ…âŒâœ…âŒâœ…âŒâœ…âŒâœ…âœ…âŒâŒâœ…âŒâœ…âœ…âœ…âœ…âŒâœ…âŒâœ…âœ…âŒâœ…âŒâŒâœ…âœ…âŒâŒâœ…âŒâŒâŒâœ…âœ…âœ…âœ…âœ…âŒâœ…âœ…âœ… [SauvegardÃ© 1750/3646]âŒâŒâœ…âŒâœ…âœ…âœ…âŒâœ…âœ…âŒâœ…âœ…âŒâœ…âœ…âœ…âœ…âŒâŒâœ…âŒâŒâœ…âœ…âŒâœ…âŒâŒâŒâœ…âœ…âŒâŒâŒâŒâŒâœ…âŒâŒâœ…âŒâœ…âŒâœ…âŒâœ…âŒâœ…âœ… [SauvegardÃ© 1800/3646]âœ…âœ…âŒâŒâŒâŒâœ…âœ…âœ…âœ…âœ…âœ…âŒâœ…âŒâœ…âœ…âŒâŒâŒâœ…âœ…âœ…âŒâŒâŒâœ…âœ…âŒâœ…âœ…âœ…âŒâœ…âŒâœ…âŒâŒâœ…âœ…âŒâŒâœ…âœ…âŒâœ…âŒâœ…âœ…âŒ [SauvegardÃ© 1850/3646]âœ…âŒâœ…âŒâœ…âœ…âœ…âœ…âœ…âœ…âŒâœ…âŒâœ…âŒâŒâœ…âŒâœ…âŒâœ…âœ…âœ…âœ…âœ…âŒâœ…âœ…âœ…âŒâŒâŒâœ…âœ…âŒâœ…âŒâœ…âœ…âŒâœ…âœ…âœ…âœ…âœ…âŒâœ…âœ…âœ…âœ… [SauvegardÃ© 1900/3646]âŒâœ…âœ…âŒâœ…âœ…âŒâŒâœ…âŒâœ…âœ…âœ…âŒâŒâŒâœ…âœ…âœ…âŒâŒâŒâœ…âœ…âŒâœ…âœ…âœ…âœ…âœ…âŒâŒâœ…âŒâŒâœ…âœ…âœ…âœ…âŒâœ…âœ…âœ…âŒâœ…âœ…âŒâŒâœ…âœ… [SauvegardÃ© 1950/3646]âŒâœ…âŒâŒâœ…âœ…âœ…âœ…âœ…âœ…âœ…âŒâœ…âœ…âœ…âœ…âŒâœ…âœ…âŒâœ…âŒâœ…âœ…âŒâŒâœ…âœ…âœ…âœ…âœ…âœ…âŒâŒâŒâœ…âœ…âœ…âœ…âŒâœ…âœ…âœ…âœ…âŒâŒâœ…âŒâœ…âŒ [SauvegardÃ© 2000/3646]âœ…âœ…âŒâœ…âœ…âŒâŒâœ…âŒâœ…âŒâŒâœ…âŒâŒâŒâœ…âŒâœ…âŒâœ…âœ…âœ…âœ…âŒâœ…âœ…âœ…âŒâœ…âœ…âŒâœ…âœ…âœ…âœ…âœ…âœ…âœ…âŒâœ…âœ…âœ…âœ…âœ…âœ…âŒâŒâŒâŒ [SauvegardÃ© 2050/3646]âœ…âœ…âœ…âœ…âœ…âŒâœ…âœ…âœ…âŒâŒâŒâœ…âœ…âŒâœ…âŒâœ…âœ…âœ…âŒâŒâœ…âŒâœ…âŒâœ…âŒâœ…âŒâœ…âœ…âœ…âŒâœ…âœ…âŒâœ…âŒâŒâœ…âœ…âœ…âŒâœ…âŒâœ…âœ…âŒâŒ [SauvegardÃ© 2100/3646]âœ…âœ…âœ…âœ…âœ…âœ…âœ…âŒâŒâœ…âœ…âœ…âŒâœ…âŒâœ…âŒâœ…âœ…âœ…âœ…âŒâŒâœ…âŒâœ…âŒâŒâœ…âœ…âœ…âœ…âŒâœ…âœ…âŒâœ…âœ…âœ…âœ…âœ…âœ…âŒâœ…âŒâŒâŒâŒâœ…âŒ [SauvegardÃ© 2150/3646]âŒâœ…âœ…âŒâŒâœ…âœ…âŒâœ…âœ…âœ…âœ…âœ…âœ…âœ…âŒâœ…âœ…âŒâœ…âŒâŒâœ…âŒâŒâŒâœ…âœ…âœ…âœ…âŒâŒâŒâœ…âŒâœ…âŒâœ…âŒâŒâŒâŒâŒâœ…âœ…âœ…âŒâœ…âŒâŒ [SauvegardÃ© 2200/3646]âŒâŒâœ…âŒâŒâœ…âŒâŒâŒâœ…âœ…âŒâœ…âœ…âœ…âœ…âœ…âœ…âŒâœ…âŒâœ…âŒâœ…âœ…âŒâŒâœ…âœ…âŒâŒâœ…âŒâœ…âŒâœ…âŒâœ…âœ…âœ…âŒâŒâœ…âŒâœ…âŒâœ…âœ…âœ…âŒ [SauvegardÃ© 2250/3646]âŒâŒâœ…âŒâŒâŒâœ…âŒâœ…âœ…âœ…âŒâŒâœ…âŒâŒâœ…âŒâœ…âœ…âœ…âœ…âŒâœ…âœ…âœ…âŒâŒâŒâœ…âœ…âŒâŒâŒâœ…âœ…âŒâŒâŒâœ…âœ…âœ…âœ…âŒâŒâœ…âŒâœ…âœ…âŒ [SauvegardÃ© 2300/3646]âŒâœ…âœ…âœ…âœ…âœ…âŒâœ…âœ…âœ…âŒâŒâŒâœ…âŒâœ…âœ…âŒâœ…âŒâŒâœ…âŒâŒâœ…âœ…âŒâœ…âœ…âœ…âŒâŒâœ…âœ…âŒâœ…âŒâœ…âœ…âœ…âœ…âŒâŒâœ…âœ…âŒâœ…âœ…âŒâœ… [SauvegardÃ© 2350/3646]âœ…âŒâœ…âœ…âœ…âœ…âŒâœ…âœ…âœ…âœ…âŒâŒâŒâœ…âœ…âœ…âœ…âœ…âœ…âŒâœ…âœ…âœ…âœ…âœ…âœ…âœ…âŒâœ…âœ…âŒâœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âŒâœ…âœ…âœ…âŒâœ…âŒ [SauvegardÃ© 2400/3646]âŒâœ…âœ…âœ…âœ…âœ…âœ…âœ…âŒâŒâŒâŒâœ…âœ…âŒâœ…âŒâŒâœ…âœ…âœ…âœ…âœ…âœ…âŒâŒâœ…âœ…âŒâœ…âŒâŒâŒâœ…âœ…âŒâœ…âŒâœ…âŒâœ…âœ…âœ…âœ…âŒâœ…âŒâœ…âœ…âŒ [SauvegardÃ© 2450/3646]âœ…âœ…âœ…âŒâœ…âœ…âœ…âŒâœ…âŒâŒâœ…âœ…âœ…âœ…âŒâœ…âœ…âŒâŒâœ…âœ…âŒâœ…âŒâœ…âœ…âœ…âœ…âŒâŒâœ…âœ…âœ…âœ…âœ…âœ…âŒâŒâŒâœ…âœ…âŒâœ…âœ…âŒâœ…âŒâœ…âœ… [SauvegardÃ© 2500/3646]âœ…âœ…âœ…âŒâœ…âœ…âŒâŒâŒâœ…âœ…âŒâœ…âœ…âœ…âœ…âœ…âŒâœ…âœ…âŒâœ…âœ…âŒâœ…âœ…âœ…âŒâœ…âœ…âœ…âœ…âŒâŒâœ…âœ…âŒâŒâœ…âŒâœ…âœ…âŒâœ…âœ…âŒâœ…âœ…âŒâœ… [SauvegardÃ© 2550/3646]âœ…âœ…âœ…âœ…âŒâœ…âŒâœ…âœ…âœ…âœ…âœ…âŒâœ…âœ…âœ…âœ…âŒâŒâœ…âŒâœ…âœ…âŒâœ…âŒâŒâœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âŒâŒâŒâŒâœ…âœ…âœ…âœ…âŒâœ…âœ…âœ…âœ…âœ…âŒ [SauvegardÃ© 2600/3646]âŒâœ…âŒâœ…âŒâŒâœ…âœ…âœ…âŒâœ…âœ…âœ…âŒâŒâœ…âœ…âœ…âœ…âœ…âŒâœ…âœ…âŒâŒâœ…âœ…âœ…âŒâœ…âœ…âœ…âŒâŒâŒâŒâœ…âœ…âŒâœ…âŒâŒâœ…âŒâŒâœ…âœ…âœ…âœ…âŒ [SauvegardÃ© 2650/3646]âŒâŒâœ…âŒâœ…âŒâœ…âœ…âŒâœ…âŒâœ…âŒâœ…âœ…âœ…âœ…âœ…âœ…âŒâœ…âœ…âŒâœ…âœ…âœ…âœ…âœ…âœ…âŒâœ…âœ…âŒâœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âŒâŒâœ…âŒâŒâœ…âœ…âœ…âŒ [SauvegardÃ© 2700/3646]âœ…âŒâŒâœ…âŒâŒâœ…âœ…âœ…âœ…âœ…âŒâŒâœ…âœ…âœ…âœ…âŒâŒâœ…âŒâŒâœ…âŒâŒâŒâŒâœ…âŒâŒâŒâœ…âŒâŒâœ…âŒâœ…âŒâœ…âœ…âœ…âœ…âœ…âœ…âŒâŒâŒâœ…âŒâœ… [SauvegardÃ© 2750/3646]âŒâŒâŒâœ…âœ…âŒâœ…âœ…âŒâœ…âŒâŒâŒâœ…âŒâŒâœ…âŒâœ…âœ…âŒâŒâœ…âŒâœ…âœ…âœ…âœ…âœ…âœ…âœ…âŒâœ…âœ…âŒâœ…âœ…âŒâœ…âœ…âœ…âœ…âœ…âŒâœ…âŒâœ…âœ…âŒâœ… [SauvegardÃ© 2800/3646]âœ…âœ…âŒâœ…âœ…âŒâŒâŒâœ…âŒâœ…âœ…âŒâœ…âŒâŒâœ…âŒâœ…âœ…âŒâœ…âœ…âœ…âœ…âŒâœ…âŒâœ…âŒâœ…âœ…âœ…âœ…âœ…âœ…âŒâŒâŒâŒâŒâœ…âŒâŒâœ…âœ…âœ…âŒâœ…âœ… [SauvegardÃ© 2850/3646]âœ…âœ…âœ…âŒâŒâŒâœ…âŒâœ…âœ…âœ…âœ…âŒâœ…âœ…âŒâœ…âŒâœ…âŒâŒâœ…âœ…âœ…âœ…âŒâœ…âŒâŒâŒâœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âŒâœ…âŒâœ…âŒâŒâŒâœ…âŒâœ… [SauvegardÃ© 2900/3646]âœ…âœ…âœ…âœ…âœ…âŒâœ…âŒâœ…âœ…âŒâœ…âœ…âœ…âœ…âœ…âŒâœ…âŒâœ…âŒâœ…âŒâœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âŒâŒâŒâœ…âŒâœ…âœ…âœ…âœ…âœ…âŒâŒâŒâœ…âœ…âœ…âœ…âœ…âœ… [SauvegardÃ© 2950/3646]âœ…âœ…âœ…âœ…âŒâœ…âœ…âŒâŒâŒâŒâŒâœ…âœ…âœ…âŒâœ…âœ…âœ…âŒâœ…âŒâœ…âœ…âŒâŒâœ…âœ…âœ…âŒâœ…âœ…âŒâœ…âœ…âŒâŒâœ…âœ…âŒâŒâœ…âœ…âœ…âœ…âŒâœ…âœ…âŒâŒ [SauvegardÃ© 3000/3646]âœ…âŒâœ…âœ…âœ…âŒâŒâœ…âœ…âœ…âŒâœ…âœ…âŒâŒâœ…âœ…âœ…âœ…âŒâœ…âœ…âœ…âŒâŒâœ…âœ…âœ…âŒâœ…âœ…âŒâŒâœ…âœ…âœ…âœ…âœ…âœ…âœ…âŒâŒâœ…âœ…âŒâŒâœ…âŒâŒâœ… [SauvegardÃ© 3050/3646]âœ…âŒâœ…âŒâœ…âœ…âŒâŒâœ…âœ…âœ…âœ…âŒâŒâŒâœ…âŒâœ…âœ…âœ…âŒâœ…âœ…âŒâŒâŒâœ…âŒâŒâŒâœ…âŒâœ…âŒâœ…âŒâœ…âŒâœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âŒâŒâŒâŒ [SauvegardÃ© 3100/3646]âŒâœ…âŒâŒâŒâŒâœ…âœ…âœ…âœ…âŒâŒâœ…âŒâŒâœ…âœ…âŒâŒâŒâœ…âœ…âŒâŒâœ…âŒâœ…âœ…âœ…âœ…âœ…âŒâœ…âŒâœ…âœ…âœ…âœ…âœ…âŒâŒâœ…âœ…âŒâœ…âœ…âœ…âœ…âœ…âœ… [SauvegardÃ© 3150/3646]âŒâŒâœ…âœ…âœ…âŒâœ…âŒâœ…âœ…âœ…âŒâœ…âœ…âœ…âœ…âœ…âœ…âŒâœ…âŒâœ…âŒâœ…âœ…âŒâŒâŒâŒâœ…âŒâœ…âœ…âŒâœ…âŒâŒâœ…âœ…âœ…âœ…âœ…âŒâœ…âœ…âœ…âŒâœ…âœ…âœ… [SauvegardÃ© 3200/3646]âŒâŒâœ…âŒâœ…âŒâŒâŒâœ…âŒâŒâŒâœ…âœ…âŒâœ…âŒâŒâŒâŒâœ…âœ…âœ…âœ…âŒâœ…âŒâŒâœ…âŒâŒâŒâœ…âœ…âœ…âŒâŒâŒâŒâŒâŒâœ…âœ…âŒâœ…âœ…âŒâŒâŒâœ… [SauvegardÃ© 3250/3646]âœ…âŒâœ…âœ…âœ…âœ…âœ…âŒâŒâœ…âŒâŒâŒâŒâœ…âŒâœ…âœ…âŒâœ…âŒâŒâœ…âœ…âŒâœ…âœ…âŒâŒâœ…âœ…âœ…âœ…âœ…âœ…âŒâœ…âŒâœ…âŒâœ…âŒâœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ… [SauvegardÃ© 3300/3646]âŒâŒâŒâŒâœ…âœ…âœ…âœ…âŒâœ…âŒâœ…âŒâŒâŒâœ…âŒâŒâœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âŒâœ…âœ…âŒâŒâŒâœ…âœ…âŒâœ…âœ…âœ…âŒâœ…âœ…âœ…âŒâŒâœ…âœ…âŒâœ… [SauvegardÃ© 3350/3646]âŒâŒâœ…âœ…âŒâœ…âœ…âŒâŒâŒâŒâŒâŒâœ…âŒâœ…âœ…âŒâŒâŒâŒâœ…âœ…âŒâœ…âœ…âœ…âœ…âœ…âœ…âŒâŒâœ…âœ…âœ…âŒâŒâœ…âœ…âœ…âŒâŒâœ…âœ…âœ…âœ…âŒâœ…âœ…âœ… [SauvegardÃ© 3400/3646]âœ…âŒâœ…âœ…âœ…âœ…âœ…âŒâŒâŒâŒâŒâœ…âœ…âŒâŒâŒâœ…âœ…âŒâœ…âœ…âœ…âŒâŒâŒâœ…âœ…âŒâŒâœ…âœ…âœ…âœ…âŒâŒâŒâœ…âœ…âŒâœ…âŒâœ…âŒâœ…âœ…âŒâŒâœ…âŒ [SauvegardÃ© 3450/3646]âœ…âœ…âœ…âœ…âœ…âœ…âŒâœ…âŒâœ…âœ…âŒâŒâŒâŒâœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âŒâœ…âœ…âœ…âœ…âŒâŒâœ…âŒâœ…âŒâŒâœ…âœ…âœ…âœ…âœ…âŒâœ…âœ…âœ…âœ…âŒâŒâœ…âœ… [SauvegardÃ© 3500/3646]âŒâœ…âŒâŒâŒâœ…âœ…âŒâŒâŒâŒâœ…âœ…âŒâŒâœ…âœ…âŒâŒâŒâœ…âœ…âŒâœ…âŒâœ…âŒâœ…âŒâŒâŒâŒâœ…âœ…âŒâœ…âœ…âŒâœ…âŒâœ…âŒâŒâœ…âœ…âœ…âŒâŒâŒâœ… [SauvegardÃ© 3550/3646]âŒâœ…âŒâœ…âœ…âŒâœ…âŒâŒâŒâœ…âœ…âœ…âœ…âŒâœ…âŒâœ…âŒâŒâœ…âŒâŒâŒâœ…âœ…âŒâŒâŒâŒâœ…âœ…âŒâœ…âŒâœ…âŒâœ…âœ…âŒâŒâœ…âŒâœ…âœ…âœ…âœ…âœ…âœ…âœ… [SauvegardÃ© 3600/3646]âŒâœ…âŒâœ…âŒâœ…âŒâœ…âŒâŒâœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âŒâŒâœ…âœ…âœ…âŒâŒâŒâœ…âœ…âœ…âŒâŒâœ…âœ…âŒâœ…âœ…âœ…âœ…âŒâŒâŒâœ…âŒ\n",
      "\n",
      "ðŸ FINI ! DurÃ©e totale : 247.5 minutes.\n",
      "ðŸ“ RÃ©sultats complets sauvegardÃ©s dans : resultats_ironie_full.json\n",
      "\n",
      "==================================================\n",
      "ðŸ“Š RAPPORT FINAL (Train + Test)\n",
      "==================================================\n",
      "Accuracy Globale: 60.29%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       IRONY       0.56      0.88      0.68      1756\n",
      "   NOT_IRONY       0.75      0.35      0.48      1890\n",
      "\n",
      "    accuracy                           0.60      3646\n",
      "   macro avg       0.65      0.61      0.58      3646\n",
      "weighted avg       0.66      0.60      0.57      3646\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from datasets import load_dataset\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "MODEL = \"qwen2.5:7b-instruct\"\n",
    "URL = \"http://localhost:11434/api/chat\"\n",
    "OUTPUT_FILE = \"resultats_ironie_full.json\"\n",
    "BACKUP_EVERY = 50  # Sauvegarde automatique tous les 50 tweets\n",
    "\n",
    "# --- 1. CHARGEMENT COMPLET (TRAIN + TEST) ---\n",
    "print(\"â³ Chargement de TOUT le dataset TweetEval (Irony)...\")\n",
    "dataset_train = load_dataset(\"tweet_eval\", \"irony\", split=\"train\")\n",
    "dataset_test = load_dataset(\"tweet_eval\", \"irony\", split=\"test\")\n",
    "\n",
    "# On fusionne tout dans une seule liste pour la nuit\n",
    "full_data = []\n",
    "print(\"ðŸ“¦ Fusion des donnÃ©es...\")\n",
    "for item in dataset_train:\n",
    "    full_data.append({\"text\": item['text'], \"label\": item['label'], \"split\": \"train\"})\n",
    "for item in dataset_test:\n",
    "    full_data.append({\"text\": item['text'], \"label\": item['label'], \"split\": \"test\"})\n",
    "\n",
    "print(f\"âœ… PrÃªt Ã  traiter {len(full_data)} tweets cette nuit.\")\n",
    "\n",
    "# --- 2. FONCTION ROBUSTE ---\n",
    "def classify_robust(text, max_retries=3):\n",
    "    payload = {\n",
    "        \"model\": MODEL,\n",
    "        \"stream\": False,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are a strict irony detector. Output JSON only.\"},\n",
    "            # Few-Shot Examples (Ta recette secrÃ¨te)\n",
    "            {\"role\": \"user\", \"content\": \"Text: I love getting stuck in traffic.\\nReturn JSON exactly: {\\\"label\\\":\\\"IRONY\\\"|\\\"NOT_IRONY\\\",\\\"confidence\\\":0-1}.\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"{\\\"label\\\":\\\"IRONY\\\", \\\"confidence\\\": 0.99}\"},\n",
    "            {\"role\": \"user\", \"content\": \"Text: The weather is beautiful today.\\nReturn JSON exactly: {\\\"label\\\":\\\"IRONY\\\"|\\\"NOT_IRONY\\\",\\\"confidence\\\":0-1}.\"},\n",
    "            {\"role\": \"assistant\", \"content\": \"{\\\"label\\\":\\\"NOT_IRONY\\\", \\\"confidence\\\": 0.99}\"},\n",
    "            # Target\n",
    "            {\"role\": \"user\", \"content\": f\"Return JSON exactly: {{\\\"label\\\":\\\"IRONY\\\"|\\\"NOT_IRONY\\\",\\\"confidence\\\":0-1}}.\\nText: {text}\"}\n",
    "        ],\n",
    "        \"options\": {\"temperature\": 0}\n",
    "    }\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            r = requests.post(URL, json=payload, timeout=120)\n",
    "            r.raise_for_status()\n",
    "            content = r.json()[\"message\"][\"content\"]\n",
    "            \n",
    "            # Parsing JSON artisanal mais solide\n",
    "            start = content.find('{')\n",
    "            end = content.rfind('}') + 1\n",
    "            if start != -1 and end != -1:\n",
    "                return json.loads(content[start:end]).get(\"label\", \"NOT_IRONY\")\n",
    "            \n",
    "            # Fallback\n",
    "            if \"IRONY\" in content and \"NOT\" not in content: return \"IRONY\"\n",
    "            return \"NOT_IRONY\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            if attempt == max_retries - 1:\n",
    "                print(f\"\\nâš ï¸ Ã‰chec dÃ©finitif sur ce tweet : {e}\")\n",
    "                return \"ERROR\"\n",
    "            time.sleep(2) # On attend un peu avant de rÃ©essayer\n",
    "\n",
    "# --- 3. EXÃ‰CUTION & SAUVEGARDE CONTINUE ---\n",
    "results = []\n",
    "# Si le fichier existe dÃ©jÃ , on reprend (optionnel, ici on Ã©crase pour faire simple)\n",
    "if os.path.exists(OUTPUT_FILE):\n",
    "    print(f\"âš ï¸ Le fichier {OUTPUT_FILE} existe dÃ©jÃ , je vais ajouter Ã  la suite...\")\n",
    "\n",
    "print(\"\\nðŸš€ C'est parti ! Bonne nuit. (Ctrl+C pour arrÃªter si besoin)\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for i, item in enumerate(full_data):\n",
    "    text = item['text']\n",
    "    true_label = \"IRONY\" if item['label'] == 1 else \"NOT_IRONY\"\n",
    "    \n",
    "    # Classification\n",
    "    pred_label = classify_robust(text)\n",
    "    \n",
    "    # On stocke tout\n",
    "    result_entry = {\n",
    "        \"id\": i,\n",
    "        \"text\": text,\n",
    "        \"true_label\": true_label,\n",
    "        \"pred_label\": pred_label,\n",
    "        \"split\": item['split']\n",
    "    }\n",
    "    results.append(result_entry)\n",
    "    \n",
    "    # Pour les mÃ©triques\n",
    "    if pred_label != \"ERROR\":\n",
    "        y_true.append(true_label)\n",
    "        y_pred.append(pred_label)\n",
    "\n",
    "    # Affichage minimaliste pour pas bourrer la console\n",
    "    status = \"âœ…\" if pred_label == true_label else \"âŒ\"\n",
    "    print(status, end=\"\", flush=True)\n",
    "    \n",
    "    # Sauvegarde intermÃ©diaire (SÃ©curitÃ©)\n",
    "    if (i + 1) % BACKUP_EVERY == 0:\n",
    "        with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "        print(f\" [SauvegardÃ© {i+1}/{len(full_data)}]\", end=\"\", flush=True)\n",
    "\n",
    "# --- 4. RÃ‰CAPITULATIF AU RÃ‰VEIL ---\n",
    "# Sauvegarde finale\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "duration = (time.time() - start_time) / 60\n",
    "print(f\"\\n\\nðŸ FINI ! DurÃ©e totale : {duration:.1f} minutes.\")\n",
    "print(f\"ðŸ“ RÃ©sultats complets sauvegardÃ©s dans : {OUTPUT_FILE}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ðŸ“Š RAPPORT FINAL (Train + Test)\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Accuracy Globale: {accuracy_score(y_true, y_pred):.2%}\")\n",
    "print(classification_report(y_true, y_pred, labels=[\"IRONY\", \"NOT_IRONY\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5801d1",
   "metadata": {},
   "source": [
    "# Dire que c'est le resultat le plus mauvais,  meme si ce modele est un modeles tres populaire comme un LLM inferable en LOCAL,  d'ailleur JINA v4 qui est un  modele d'embedding SOTA, recement sortie (07/2025,  n'oublie pas maintenant  c'est 01/2026 don c c'est un  modele tres recent) est \"Built on Qwen/Qwen2.5-VL-3B-Instruct\", utilise que, il est meme tres complexe e espace et en  temps. Explique finalement en  2  lignes pourquoi "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80780ad2",
   "metadata": {},
   "source": [
    "E"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
